{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPLuT2nAXYFgzkD7HJe6ZDh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haru1489248/nlp-100-nock/blob/main/ch09/section_88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 88. 極性分析\n",
        "問題87でファインチューニングされたモデルを用いて、以下の文の極性を予測せよ。\n",
        "\n",
        "“The movie was full of incomprehensibilities.”\n",
        "\n",
        "“The movie was full of fun.”\n",
        "\n",
        "“The movie was full of excitement.”\n",
        "\n",
        "“The movie was full of crap.”\n",
        "\n",
        "“The movie was full of rubbish.”\n",
        "\n"
      ],
      "metadata": {
        "id": "oRZM9QyEuTFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vpdS6uTaiAM",
        "outputId": "156fa235-0c3f-495e-ddd3-03974d1eef65"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Collecting transformers\n",
            "  Downloading transformers-5.0.0-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Collecting huggingface-hub<2.0,>=1.3.0 (from transformers)\n",
            "  Downloading huggingface_hub-1.3.5-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading transformers-5.0.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-1.3.5-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub, transformers, evaluate\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.36.0\n",
            "    Uninstalling huggingface-hub-0.36.0:\n",
            "      Successfully uninstalled huggingface-hub-0.36.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.6\n",
            "    Uninstalling transformers-4.57.6:\n",
            "      Successfully uninstalled transformers-4.57.6\n",
            "Successfully installed evaluate-0.4.6 huggingface-hub-1.3.5 transformers-5.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkHJl3U4aJ-k",
        "outputId": "c8c13c3b-303e-40bf-90c5-8470fabe358f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "import evaluate # hugging face公式のライブラリ"
      ],
      "metadata": {
        "id": "fV-o2Bf_aaqt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SST2Dataset(Dataset):\n",
        "  def __init__(self, sentences, labels, tokenizer):\n",
        "    super().__init__()\n",
        "    self.encodings = tokenizer(sentences, truncation=True) # paddingはcollator側でやる\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # items()はPythonのdictのメソッド。\n",
        "    # encodingsはdict互換のBatchEncodingオブジェクトなのでitems()が使える\n",
        "    item = {k: v[idx] for k, v in self.encodings.items()}\n",
        "    item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "    return item"
      ],
      "metadata": {
        "id": "ov3bkh7FwaAi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "taUKne48xIQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_SST2(path):\n",
        "  sentences, labels = [], []\n",
        "  with open(path, 'r') as f:\n",
        "    reader = csv.DictReader(f, delimiter='\\t')\n",
        "    for row in reader:\n",
        "      sentences.append(row['sentence'])\n",
        "      labels.append(int(row['label']))\n",
        "  return sentences, labels"
      ],
      "metadata": {
        "id": "NfAL1FBuxLxy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_src = '/content/drive/MyDrive/SST-2/train.tsv'\n",
        "dev_src = '/content/drive/MyDrive/SST-2/dev.tsv'"
      ],
      "metadata": {
        "id": "e1yXqSokcDrP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, train_labels = load_SST2(train_src)\n",
        "dev_sentences, dev_labels = load_SST2(dev_src)"
      ],
      "metadata": {
        "id": "aLegN46GcBWA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SST2Dataset(train_sentences, train_labels, tokenizer)\n",
        "dev_dataset = SST2Dataset(dev_sentences, dev_labels, tokenizer)"
      ],
      "metadata": {
        "id": "3eVs6GHCdL-A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer) # batchごとに最大長に合わせてpaddingしてくれるcollator"
      ],
      "metadata": {
        "id": "lnYvozVixfHy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2 # クラス数\n",
        ")"
      ],
      "metadata": {
        "id": "WkUBV9QQx7QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate.load(\"accuracy\")"
      ],
      "metadata": {
        "id": "5w-vxwiryGS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### compute_matricsとは？\n",
        "評価時に使う指標（ここでは正解率）を計算する関数\n",
        "- eval_predとは?\n",
        "  - tupleで中身は(logits: モデルの出力, labels: 正解ラベル)となっている\n",
        "  - logits.shape: (N, num_labels)\n",
        "    - 各サンプルに対するクラスごとのスコア\n",
        "  - labels.shape: (N)\n",
        "    - 正解ラベル(0 or 1)\n",
        "- logitsとは？\n",
        "  - BERTの分類モデルは`outputs = model(...)`; `outputs.logits`を返す\n",
        "  - 例（2クラス）:\n",
        "\n",
        "```\n",
        "logits = [\n",
        "  [2.3, -0.8], # sample 1\n",
        "  [-1.1, 0.4], # sample 2\n",
        "]\n",
        "```\n",
        "  - softmax前のスコア\n",
        "  - 大きい方がモデルの予測クラス\n",
        "- `np.argmax(logits, axis=-1)`とは？\n",
        "  - 各サンプルについて一番スコアが高いクラスのインデックスを取る\n",
        "  ```\n",
        "  [2.3, -0.8] -> 0\n",
        "  [-1.1, 0.4] -> 1\n",
        "  ```\n",
        "\n"
      ],
      "metadata": {
        "id": "6IUP2BTG3F_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "  logits, labels = eval_pred\n",
        "  preds = np.argmax(logits, axis=-1)\n",
        "  return accuracy.compute(predictions=preds, references=labels)"
      ],
      "metadata": {
        "id": "IMKQyaDQzYYv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TrainingArgumentsとは？\n",
        "trainerに渡す学習の設定\n",
        "- output_dir: 学習結果の保存先ディレクトリ\n",
        "- eval_strategy: いつ評価するか\n",
        "  - `\"epoch\"`: 1エポック終わるごとに評価\n",
        "  - `\"steps\"`: 一定ステップごと\n",
        "  - `\"no\"`: 評価しない\n",
        "- save_strategy: いつモデルを保存するか\n",
        "- learning_rate: 学習率（2e-5=2*10^{-5}）\n",
        "- per_device_train_batch_size: 1GPU（or CPU）あたりの学習サイズ\n",
        "- per_device_eval_batch_size: 評価時のバッチサイズ（勾配計算しないので、学習時より大きくてもいい）\n",
        "- num_train_epochs: データを何周するか\n",
        "- weight_decay: L2正則化。重みが大きくなすぎるのを防ぐ（正則化項の係数）\n",
        "- loging_steps: 何ステップごとにログを出すか（lossやlearning_rateを表示）\n",
        "1ステップ=一回のoptimizer更新（だいたい1バッチ処理）"
      ],
      "metadata": {
        "id": "qroJBGIEz6yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"sst2-bert\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        ")"
      ],
      "metadata": {
        "id": "4nII_vffyPLu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AxdpqzOm20-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "o4qrP3tky7_t"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "result = trainer.evaluate()\n",
        "print(result) # eval_accuracyが出る"
      ],
      "metadata": {
        "id": "96MuE-08zsku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"The movie was full of incomprehensibilities.\",\n",
        "    \"The movie was full of fun.\",\n",
        "    \"The movie was full of excitement.\",\n",
        "    \"The movie was full of crap.\",\n",
        "    \"The movie was full of rubbish.\"\n",
        "]"
      ],
      "metadata": {
        "id": "A7vZQcWHJKc2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encodings = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "edejqnpSJc8p"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "with torch.no_grad():\n",
        "  logits = model(**encodings.to(device)).logits\n",
        "  pred_ids = torch.argmax(logits, dim=-1) # dim=-1で最後の次元を指定している（最後の次元はpositive, negative class）\n",
        "  prods = torch.softmax(logits, dim=-1) # 合計が1になる確率を作成している"
      ],
      "metadata": {
        "id": "YccibOpMJzQk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}"
      ],
      "metadata": {
        "id": "OAFRL1FwKbXA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text, pid, pr in zip(sentences, pred_ids, prods):\n",
        "  neg, pos = pr.tolist()\n",
        "  print(f\"{text}\\n pred={id2label[int(pid)]} (NEG={neg:.3f}, POS={pos:.3f})\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMKfDT4QKgvm",
        "outputId": "96b8c338-06ac-4001-c504-d42e2a3bc39c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The movie was full of incomprehensibilities.\n",
            " pred=NEGATIVE (NEG=0.998, POS=0.002)\n",
            "\n",
            "The movie was full of fun.\n",
            " pred=POSITIVE (NEG=0.000, POS=1.000)\n",
            "\n",
            "The movie was full of excitement.\n",
            " pred=POSITIVE (NEG=0.000, POS=1.000)\n",
            "\n",
            "The movie was full of crap.\n",
            " pred=NEGATIVE (NEG=0.999, POS=0.001)\n",
            "\n",
            "The movie was full of rubbish.\n",
            " pred=NEGATIVE (NEG=0.999, POS=0.001)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}