{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiBgFEKNdRiM9Xn8muRQdk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haru1489248/nlp-100-nock/blob/main/ch08/section_76.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 76. ミニバッチ学習\n",
        "75のパディングの処理を活用して、ミニバッチでモデルを学習する\n",
        "### フロー\n",
        "1. Dataset(SST-2, train/dev)の読み込み\n",
        "2. Datasetに含まれる語彙の取得\n",
        "3. 単語埋め込み行列, key-index辞書の作成\n",
        "4. Datasetの前処理(token->idに変換)\n",
        "5. DataLoaderの作成\n",
        "6. パディング処理\n",
        "7. 学習"
      ],
      "metadata": {
        "id": "UVBinvUIX1zO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PilSLpzqqblA",
        "outputId": "595288b4-8830-471b-cdcd-72b511f6184c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxDFgpIJXvUx",
        "outputId": "934db2dc-5001-4f4e-ae2d-c9c947dcde08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from typing import Any, Dict, List, Set, Tuple, Union\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from gensim.models import KeyedVectors\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm.notebook import tqdm # colabだとtqdm.notebookを使用するらしい"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def padding(input_ids: torch.Tensor, max_length: int, pad_value: int) -> torch.Tensor:\n",
        "    current_length = input_ids.size(0)\n",
        "    # torch.full()は指定されたfill_valueを使用して大きさsize分埋めてtensorを返す\n",
        "    # 基本形 torch.full(size, fill_value, *, dtype=None, device=None)\n",
        "    base = torch.full((max_length,), pad_value, dtype=input_ids.dtype, device=input_ids.device)\n",
        "    # current_lengthまでinput_idsで埋める\n",
        "    base[:current_length] = input_ids\n",
        "    return base"
      ],
      "metadata": {
        "id": "nA6MdF2tqEmn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate(\n",
        "    batch: List[Dict[str, Union[torch.Tensor, str]]], pad_value: int = 0\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    lengths = [len(item[\"input_ids\"]) for item in batch]\n",
        "    # keyはどのようにソートするかを関数で指定する引数\n",
        "    # 今回は降順なのでinput_idsの長さが長いものから順番に並べられる\n",
        "    # xはbatchの長さのrangeを順番に入れているのでbatch分のindexを作成する\n",
        "    sorted_batch = sorted(\n",
        "        batch, key=lambda x: len(x[\"input_ids\"]), reverse=True\n",
        "    )  # True = 降順指定\n",
        "    max_length = max(lengths)\n",
        "\n",
        "    padded_input_ids = torch.stack(\n",
        "        [padding(item[\"input_ids\"], max_length, pad_value) for item in sorted_batch]\n",
        "    )\n",
        "\n",
        "    # tensor.view(-1) は要素数を保ったまま強制的にフラット（1次元）にするメソッド\n",
        "    # 例: torch.tensor([[1, 2, 3], [4, 5, 6]]).view(-1)\n",
        "    #     -> tensor([1, 2, 3, 4, 5, 6])\n",
        "    #\n",
        "    # squeeze() は shape が 1 の次元だけを削除するメソッド\n",
        "    # 上の例は shape が (2, 3) なので、squeeze() を呼んでも変化しない\n",
        "    #\n",
        "    # [0] で label を 0次元Tensor（スカラーTensor）にしている\n",
        "    # 例: tensor([0.]) -> tensor(0.)\n",
        "    labels = torch.stack([item[\"label\"].view(-1)[0] for item in sorted_batch])\n",
        "\n",
        "    return {\"input_ids\": padded_input_ids, \"label\": labels}\n"
      ],
      "metadata": {
        "id": "I6te789uqG8T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SSTDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset Class for the SST-2.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data: List[Dict[str, torch.Tensor]], embedding_matrix: torch.Tensor) -> None:\n",
        "        super().__init__() # ほとんど何もしていないらしいが継承クラスを初期化する慣習\n",
        "        self.data = data\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        object = self.data[index]\n",
        "        return object"
      ],
      "metadata": {
        "id": "QjKq-E0_Zwaz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "deviceが混在する可能性があるのでexampleのlinear1に直接deviceを渡す方法は削除し、model.to(device)でdevice設定することにした"
      ],
      "metadata": {
        "id": "1WuUYCEQfnsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SemanticClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Bag of words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dimension: int, n_classes: int) -> None:\n",
        "        super().__init__()\n",
        "        self.in_dimension = in_dimension\n",
        "        self.n_classes = n_classes\n",
        "        self.linear1 = nn.Linear(in_features=in_dimension, out_features=1, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # squeeze(1) は「index が 1 の軸のサイズが 1 のときだけ、その軸を削除して次元を 1 つ減らす」\n",
        "        return self.sigmoid(self.linear1(x)).squeeze(1)"
      ],
      "metadata": {
        "id": "yc7iLqwxbiDX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_seeds(seed: int) -> None:\n",
        "    \"\"\"Fix seeds, Pytorch, random, numpy.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    seed : int\n",
        "        Number of a seed.\n",
        "    \"\"\"\n",
        "    # random.random()・random.shuffle()などに影響する\n",
        "    # 今回は使用されていなそうだが慣習で設定するようにしている\n",
        "    random.seed(seed)\n",
        "\n",
        "    # おそらく間違いで書かれていたコード\n",
        "    # これはseed付き乱数生成器を作っているだけで使っていない（インスタンス化している）\n",
        "    # npのseedは使われていなかったので大丈夫だった\n",
        "    np.random.RandomState(seed)\n",
        "    #正解はグローバルなnumpyの乱数を固定するために定義したこれ\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # PyTorch（CPU）の乱数を固定する\n",
        "    torch.manual_seed(seed)\n",
        "    # GPUの場合\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "    # GPU内で同じ入力の場合に同じ出力を保証する設定\n",
        "    # cuDNNはNVIDIAが提供するGPU用の高速数値計算ライブラリ\n",
        "    # PyTorchでは内部でこれを使用してLinear、Conv、RNNなどを高速化している\n",
        "    # 計算速度を多少遅くするが結果が必ず同じアルゴリズムだけを使用するようにできる\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "nBmzD1rJcTZm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_matrix(\n",
        "    word_embedding_model_path: Union[str, Path],\n",
        "    vocabulary: Set[str],\n",
        ") -> Tuple[Dict[str, int], torch.Tensor]:\n",
        "    \"\"\"Extract a matrix from the pre-trained word embedding vector.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    word_embedding_model_path : Union[str, Path]\n",
        "        Path to the pre-trained word embedding model\n",
        "    start_index : int, optional\n",
        "        Starting index for the vocabulary, by default 0\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[torch.Tensor, Dict[str, int]]\n",
        "        Embedding matrix and word to index mapping\n",
        "\n",
        "    Reference\n",
        "    ---------\n",
        "    https://github.com/upura/nlp100v2025/blob/main/ch08/ans73.py#L88C1-L91C57\n",
        "    \"\"\"\n",
        "    wv_from_bin = KeyedVectors.load_word2vec_format(word_embedding_model_path, binary=True)\n",
        "\n",
        "    # \"<PAD>\"は予約語\n",
        "    key_to_idx = {\"<PAD>\": 0}\n",
        "\n",
        "    # 単語埋め込み行列の取得\n",
        "    # 最初の行は<PAD>用\n",
        "    # wv_from_bin.vectorsで行列を取り出す（行が単語、列が埋め込み次元）\n",
        "    # .shapeは(単語数, 埋め込み次元数)のtupleを返す\n",
        "    _, d_emb = wv_from_bin.vectors.shape\n",
        "    # E: List[Tensor]  （各 Tensor は shape=(d_emb,)）\n",
        "    # 中身は\n",
        "    # E = [\n",
        "    #.   tensor([0.0, 0.0, 0.0, ..., 0.0]) 長さ300\n",
        "    # ]\n",
        "    E = [torch.zeros(d_emb, dtype=torch.float32)]\n",
        "\n",
        "    # 単語が学習済み単語ベクトルに含まれているときのみ、ベクトルを取得\n",
        "    # vocabularyは語彙のiterable\n",
        "    # Setがなので集合で同じ要素を一回しか持てないデータ構造: {\"a\", \"b\", \"c\"}\n",
        "    for word in vocabulary:\n",
        "        # wv_from_bin.key_to_indexは対応する語彙の行のindexを返す\n",
        "        # wv_from_bin[word]と直接するとなかった場合にKeyErrorが返ってくる可能性があるので存在チェックしている\n",
        "        if word in wv_from_bin.key_to_index:\n",
        "            # word を key、対応する行番号（index）を value に持つ辞書を作成する\n",
        "            # len(key_to_idx) は現在の要素数なので、次に割り当てる index として正しい\n",
        "            # （すでに <PAD> が index=0 として入っている前提）\n",
        "            key_to_idx[word] = len(key_to_idx)\n",
        "\n",
        "            # word に対応する埋め込みベクトル（1単語=1行）を取得し、\n",
        "            # 埋め込み行列 E（リスト）の末尾に追加する\n",
        "            E.append(torch.tensor(wv_from_bin[word]))\n",
        "\n",
        "    # torch.stack()とは？\n",
        "    # 同じ shape の Tensor を新しい次元を作って重ねるメソッド\n",
        "    # E は (d_emb,) の Tensor を要素にもつ Python の list\n",
        "    # 結果として (vocab_size, d_emb) の埋め込み行列になる\n",
        "    # embedding_matrixはtensor型でtensor型はrequires_gradのboolean値を持っている（デフォルトはFalse）\n",
        "    # これをTrueにするとnn.Module.parameters()で取得できるようになり、最適化アルゴリズムの中で更新される\n",
        "    # 今回の課題では単語埋め込み行列の値を固定しないといけないのでデフォルトのFalseのままにする\n",
        "    embedding_matrix = torch.stack(E)\n",
        "\n",
        "    return key_to_idx, embedding_matrix"
      ],
      "metadata": {
        "id": "rIFxoMsKckXH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(row: pd.Series, key_to_idx: Dict[str, int]) -> Tuple[Dict[str, Any], int]:\n",
        "    \"\"\"Convert inputted text and label to dict object.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    row : pd.Series\n",
        "        Row of the dataset.\n",
        "    key_to_idx : Dict[str, int]\n",
        "        Dictionary of word to index.\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[Dict[str, Any], int]\n",
        "        Tokenized data dictionary and token count\n",
        "    \"\"\"\n",
        "    sentence = row[\"sentence\"]\n",
        "    label = row[\"label\"]\n",
        "    input_ids = []\n",
        "\n",
        "    for word in sentence.lower().split():\n",
        "        if word in key_to_idx:\n",
        "            input_ids.append(key_to_idx[word])\n",
        "\n",
        "    token_dict = {\n",
        "        \"text\": sentence,\n",
        "        \"label\": torch.tensor(label, dtype=torch.long),\n",
        "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "    }\n",
        "\n",
        "    return token_dict, len(input_ids)\n"
      ],
      "metadata": {
        "id": "5uQkmFcCcycZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_token(df: pd.DataFrame, key_to_idx: Dict[str, int]) -> List[Dict[str, torch.Tensor]]:\n",
        "    \"\"\"Apply tokenize function to each row of the dataframe.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Dataset dataframe.\n",
        "    key_to_idx : Dict[str, int]\n",
        "        Dictionary of word to index.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[Dict[str, torch.Tensor]]\n",
        "        List of tokenized data dictionaries\n",
        "    \"\"\"\n",
        "    # sentenceのindex化\n",
        "    # argsとはapplyで呼ばれる関数にrow以外で追加で渡したい引数を指定するもの\n",
        "    # tupleで指定している理由\n",
        "    # argsはtupleで渡さないといけないので(key_to_idx,)としている:(key_to_idx)だとint\n",
        "    tokenized_data = df.apply(tokenize, args=(key_to_idx,), axis=1)\n",
        "\n",
        "    # token数が0の行を除く\n",
        "    #.token_count = input_idsで単語ベクトルが存在しないものをフィルターしている\n",
        "    result = [token_dict for token_dict, token_count in tokenized_data if token_count > 0]\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "GK0hfGzVdEaT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocabulary(sentence: List[str]) -> Set[str]:\n",
        "    \"\"\"Get the set of vocabulary in the dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sentence : List[str]\n",
        "        List of texts.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Set[str]\n",
        "    \"\"\"\n",
        "    result = set()\n",
        "\n",
        "    for item in sentence:\n",
        "        result.update(item.lower().split())\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "1o-7NLcXdGli"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_mean_embedding(\n",
        "    embedding_matrix: torch.Tensor,\n",
        "    input_ids: torch.Tensor, # shape = (B, L)\n",
        "    pad_value: int = 0,\n",
        "    ) -> torch.Tensor:\n",
        "  # embeddings.shape = (B, L, D)\n",
        "  # B: バッチサイズ\n",
        "  # L: トークン数（系列長、padding後の長さ）\n",
        "  # D: 埋め込み次元（特徴量数）\n",
        "  embeddings = embedding_matrix[input_ids]\n",
        "\n",
        "  # pad_value でないところを 1(True)、pad_value のところを 0(False) とする tensor を作成\n",
        "  # unsqueeze(-1) は、embedding の次元 (B, L, D) に合わせるために軸を1つ追加している\n",
        "  mask = (input_ids != pad_value).unsqueeze(-1)\n",
        "  mask = mask.to(embeddings.dtype) # floatに変換\n",
        "\n",
        "  # padの埋め込みベクトルを0にする\n",
        "  embeddings = embeddings * mask\n",
        "\n",
        "  # dim=1で列方向（横方向）で平均される\n",
        "  summed = embeddings.sum(dim=1)\n",
        "\n",
        "  # clamp() は、min を指定した場合、合計した結果が min 未満なら min に置き換える\n",
        "  # max を指定した場合は、結果が max を超えたら max に置き換える\n",
        "  counts = mask.sum(dim=1).clamp(min=1.0)\n",
        "  mean_embeddings = summed / counts\n",
        "\n",
        "  return mean_embeddings"
      ],
      "metadata": {
        "id": "v0E-KNRzyM96"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    model: SemanticClassifier, # ロジスティック回帰モデル\n",
        "    embedding_matrix: torch.Tensor,\n",
        "    trainloader: DataLoader,\n",
        "    devloader: DataLoader,\n",
        "    optimizer: optim.Adam, # momentumとRMSPropを組み合わせた手法\n",
        "    criterion: nn.BCELoss, # Binary Cross Entropy Loss（2値交差エントロピー損失）\n",
        "    epoch: int, # current epoch\n",
        "    epochs: int, # total epoch\n",
        "    # torch.deviceはTensorをどこに載せるかを型安全に指定できるclassで\n",
        "    # indexをつけることによって複数のGPUを設定することができる\n",
        "    device: Union[str, torch.device] = \"cpu\",\n",
        ") -> None:\n",
        "    \"\"\"Train the model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : SemanticClassifier\n",
        "        Model to train.\n",
        "    trainloader : DataLoader\n",
        "        DataLoader for training.\n",
        "    optimizer : optim.Adam\n",
        "        Optimizer for training.\n",
        "    criterion : nn.BCELoss\n",
        "        Loss function for training.\n",
        "    epoch : int\n",
        "        Current epoch.\n",
        "    epochs : int\n",
        "        Total number of epochs.\n",
        "    device : Union[str, torch.device], optional\n",
        "        Device to use for training.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    nn.Module.train()の説明\n",
        "        呼び出すとmodeがTrueにセットされる(true=training mode)\n",
        "        mode=Trueで挙動が変わるもの\n",
        "        - torch.nn.Dropout\n",
        "            学習時に毎回特徴を一定の確率(p:引数に入れられる)で落とすという処理をして\n",
        "            同じ特徴だけ学習される過学習を防ぐ\n",
        "            評価時はDropoutの出力はそのままになる（特徴を落とさない）\n",
        "        - torch.nn.BatchNorm2d\n",
        "            詳しくはまた今度学習する（正規化に使う平均・分散などが絡んでいる内容だった）\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # 学習の様子を可視化する為に用意\n",
        "    total_loss = 0.0 # バッチの損失の合計\n",
        "    num_batches = 0 # 足し合わせた回数\n",
        "\n",
        "    # trainloader（DataLoader）は、Datasetを材料にして`__getitem__`と`__len__`を自動で呼びながら\n",
        "    # バッチ単位のデータを返してくれる仕組み\n",
        "    # DataLoaderがやっていること\n",
        "    # 1. len(train_dataset)を呼んでデータ数を調べる\n",
        "    # 2. indexのリストを作成する：[0, 1, 2, ..., N-1]\n",
        "    # 3. shuffle=Trueならindexをshuffleする\n",
        "    # 4. batch_sizeごとにindexを区切る\n",
        "    # 5. 各indexに対してdataset[i]を呼ぶ\n",
        "    # 6. 返ってきたデータをまとめて1バッチにする\n",
        "    # 7. forループに1バッチずつ渡す\n",
        "    # ※ dataloaderの instanceを作成するときにdataset, batch, shuffle(boolean)などを設定する\n",
        "    with tqdm(trainloader, desc=f\"Epoch {epoch + 1}/{epochs}\") as t:\n",
        "        # 1バッチ（batch_size件）ごとにmean_embedding, labelを取り出す\n",
        "        for batch in t:\n",
        "            mean_embedding = generate_mean_embedding(embedding_matrix, batch['input_ids'])\n",
        "            # Tensorを指定したdevice（CPU/GPU）に移動する（同じdeviceの場合はそのまま返る）\n",
        "            mean_embedding = mean_embedding.to(device)\n",
        "            # BCELoss（やSigmoidの出力）は「確率」を扱うのでfloat32を指定している\n",
        "            label = batch['label'].to(device).to(torch.float32)\n",
        "\n",
        "            # optimizerの初期化\n",
        "            # 前のバッチで計算された「勾配」を全て0にリセットする\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 推論\n",
        "            # 内部でmodel.__call__()が呼ばれる\n",
        "            # nn.Module.__call__()がforwardを呼び出すのでpredが返る\n",
        "            pred = model(mean_embedding)\n",
        "\n",
        "            # 損失値の算出\n",
        "            # 損失は各バッチごとに算出される\n",
        "            loss = criterion(pred, label)\n",
        "\n",
        "            # 損失値を基にした勾配の計算\n",
        "            # ここでは ∂loss/∂W（損失関数を重み行列で偏微分したもの）が\n",
        "            # model.linear1.weight.grad に Tensor として格納される\n",
        "            loss.backward()\n",
        "\n",
        "            # 勾配を基にAdamアルゴリズムを用いて重み更新\n",
        "            optimizer.step()\n",
        "\n",
        "            # 損失値の記録\n",
        "            # loss.item()でPyTorchのTensorをPythonのfloatに変換する\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # バッチごとの損失を表示する\n",
        "            t.set_postfix(train_loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "        # dev datasetでの評価\n",
        "        # avg_train_lossで1エポックの平均損失を計算\n",
        "        avg_train_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
        "        dev_loss, dev_accuracy = evaluate(model, embedding_matrix, devloader, criterion, device)\n",
        "\n",
        "        # t.set_postfixとは\n",
        "        # プログレスバーの表示内容を更新する関数\n",
        "        # 全体結果を表示している\n",
        "        # .4fで小数点第4位まで表示\n",
        "        t.set_postfix(train_loss=f\"{avg_train_loss:.4f}\", dev_loss=f\"{dev_loss:.4f}\", dev_acc=f\"{dev_accuracy:.4f}\")\n",
        "\n",
        "    # エポック終了時の詳細表示\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs} Summary:\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"  Dev Loss: {dev_loss:.4f}\")\n",
        "    print(f\"  Dev Accuracy: {dev_accuracy:.4f} ({dev_accuracy * 100:.2f}%)\")"
      ],
      "metadata": {
        "id": "MUhyRsq6dJPa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(\n",
        "    model: SemanticClassifier,\n",
        "    embedding_matrix: torch.Tensor,\n",
        "    devloader: DataLoader,\n",
        "    criterion: nn.BCELoss,\n",
        "    device: Union[str, torch.device] = \"cpu\",\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Evaluate the model on dev dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : SemanticClassifier\n",
        "        Model to evaluate.\n",
        "    devloader : DataLoader\n",
        "        DataLoader for evaluation.\n",
        "    criterion : nn.BCELoss\n",
        "        Loss function for evaluation.\n",
        "    device : Union[str, torch.device], optional\n",
        "        Device to use for evaluation.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[float, float]\n",
        "        Average loss and accuracy on dev dataset.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    # torch.no_grad()とは？\n",
        "    # 推論・評価用の処理なので、計算グラフを作らず、勾配を計算・追跡しないようにしている\n",
        "    # 評価では学習しないため loss.backward() や optimizer.step() を呼ばない\n",
        "    # 計算グラフを作成しないことで、メモリ使用量を抑え処理を高速化できる\n",
        "    # 計算グラフ（computational graph）\n",
        "    #\n",
        "    # 順伝播で行った計算の依存関係を記録したDAG（有向・非巡回な関係）。\n",
        "    # `requires_grad=True` のTensorを使うと自動で構築される。\n",
        "    # `loss.backward()` でグラフを逆にたどり、勾配（∂loss/∂param）を計算して `param.grad` に入れる。\n",
        "    # 学習時は中間値を保持するためメモリを使う。\n",
        "    # 推論・評価では `torch.no_grad()` により計算グラフを作らず高速化できる。\n",
        "    with torch.no_grad():\n",
        "        for batch in devloader:\n",
        "            mean_embedding = generate_mean_embedding(embedding_matrix, batch['input_ids']).to(device)\n",
        "            label = batch[\"label\"].to(device).to(torch.float32)\n",
        "\n",
        "            pred = model(mean_embedding)\n",
        "            loss = criterion(pred, label)\n",
        "\n",
        "            # pred.squeeze() でサイズが 1 の次元を削除する（labelのshapeと揃えるため）\n",
        "            # 今回は (batch_size=32, 1) の Tensor を (32,) にする\n",
        "            # 値は変わらず、Tensor の shape だけが変わる\n",
        "            # >= 0.5 で確率を True / False に二値化する\n",
        "            # float() で bool Tensor (True/False) を 1.0 / 0.0 に変換する\n",
        "            pred_binary = (pred.squeeze() >= 0.5).float()\n",
        "\n",
        "            # pred_binaryとlabelのtensorが一致する件数を合計する\n",
        "            # item()は0次元のTensorをPythonの数値（int or float）に変換する\n",
        "            correct += (pred_binary == label).sum().item()\n",
        "\n",
        "            # size(0)でindexが0番目の次元の長さを取得する（バッチ内の件数）\n",
        "            total += label.size(0)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
        "\n",
        "    # 正解率の計算\n",
        "    # 正解率 = 正解数 / 合計数\n",
        "    accuracy = correct / total if total > 0 else 0.0\n",
        "\n",
        "    return avg_loss, accuracy\n"
      ],
      "metadata": {
        "id": "_PXMll2edNBv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args) -> None:\n",
        "    fix_seeds(args.seed)\n",
        "\n",
        "    # 1. Datasetの読み込み\n",
        "    train_df = pd.read_csv(\"/content/drive/MyDrive/SST-2/train.tsv\", sep=\"\\t\")\n",
        "    dev_df = pd.read_csv(\"/content/drive/MyDrive/SST-2/dev.tsv\", sep=\"\\t\")\n",
        "\n",
        "    # 2. Datasetに含まれる語彙の取得\n",
        "    vocabulary = get_vocabulary(train_df[\"sentence\"].tolist())\n",
        "\n",
        "    # vocabulary.update()とは？\n",
        "    # vocabularyはset型のstrを持つデータ構造\n",
        "    # set.update(iterable)とすると他のiterableの要素を全部追加する（重複は自動で排除される）\n",
        "    vocabulary.update(get_vocabulary(dev_df[\"sentence\"].tolist()))\n",
        "\n",
        "    # 3. 単語埋め込み行列, key-index辞書の作成\n",
        "    key_to_idx, embedding_matrix = create_embedding_matrix(\n",
        "        \"/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz\", vocabulary\n",
        "    )\n",
        "\n",
        "    # 4. Datasetの前処理(token->idに変換)\n",
        "    train_data = convert_to_token(train_df, key_to_idx)\n",
        "    dev_data = convert_to_token(dev_df, key_to_idx)\n",
        "\n",
        "    train_dataset = SSTDataset(train_data, embedding_matrix)\n",
        "    dev_dataset = SSTDataset(dev_data, embedding_matrix)\n",
        "\n",
        "    if args.dryrun:\n",
        "        print(\"dryrun. only 1 epoch.\")\n",
        "        epochs = 1\n",
        "    else:\n",
        "        epochs = args.epochs\n",
        "\n",
        "    # 5. DataLoaderの作成\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, collate_fn=collate, shuffle=True)\n",
        "    dev_loader = DataLoader(dev_dataset, batch_size=args.batch_size, collate_fn=collate, shuffle=False)\n",
        "\n",
        "    # 6. 学習\n",
        "    # cpuで実行しないといけないのでcpuで実行するように指定する\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "    # embedding_matrix.size(1)で列の次元数を渡している（特徴量数）\n",
        "    # model.to(device)でtypesにより、モデルが保持する前パラメータ・バッファを\n",
        "    # 利用可能な device(GPU / CPU) へ移動する\n",
        "    # バッファとは学習で更新されないパラメータのこと\n",
        "    model = SemanticClassifier(in_dimension=embedding_matrix.size(1), n_classes=2).to(device)\n",
        "\n",
        "    # torch.optim.Adamのinstanceを作成する\n",
        "    # model.parameters()で学習で更新したパラメーター一覧（iterable)\n",
        "    # 更新すべきパラメータをoptimizerに教えるために渡している\n",
        "    # lrは学習率\n",
        "    # betas=(b_1, b_2)はデフォルトで(0.9, 0.999)で設定される\n",
        "    # betasは過去をどれだけ注視するかの値（notionに詳しく書いた）\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    criterion = torch.nn.BCELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train(\n",
        "            model=model,\n",
        "            embedding_matrix=embedding_matrix,\n",
        "            trainloader=train_loader,\n",
        "            devloader=dev_loader,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            epoch=epoch,\n",
        "            epochs=epochs,\n",
        "            device=device,\n",
        "        )"
      ],
      "metadata": {
        "id": "2vicRRohdiIP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # pythonスクリプトをコマンドラインから実行するときに渡された引数（args）を定義通りに解釈・解析（parse）するための# 標準ライブラリ\n",
        "    import argparse\n",
        "\n",
        "    # ArgumentParserのインスタンスを作成する（argumentを登録するために必要）\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add_argument(短い名前（省略可）, 長い名前, type・defaultなどの解釈ルール設定)\n",
        "    parser.add_argument(\"-s\", \"--seed\", type=int, default=29)\n",
        "    parser.add_argument(\"-e\", \"--epochs\", default=100, type=int)\n",
        "    parser.add_argument(\"-b\", \"--batch_size\", default=32, type=int)\n",
        "    parser.add_argument(\"-p\", \"--postfix\", type=str) # 今の所使用していない\n",
        "\n",
        "    # store_trueはそのオプションが指定されたらTrue、指定されなかったらFalseにする\n",
        "    parser.add_argument(\"--dryrun\", action=\"store_true\")\n",
        "\n",
        "    # 内部でimport sysをしているのでimport不要\n",
        "    # sys.argvを読んで登録したルールに従って解析をする\n",
        "    # sys.argvの出力例\n",
        "    # ['train.py', '-e', '50', '--dryrun']\n",
        "    # colabでは特有のargsを渡してくるので定義したargsのみ受け取るようにする\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    main(args)"
      ],
      "metadata": {
        "id": "lPugSbsQmZQV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}