{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRLekznqB/VKXIYbCPk0ns",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haru1489248/nlp-100-nock/blob/main/ch08/section_74.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 74. 開発セットにおける正解率を求める\n",
        "- 必要なもの\n",
        "  - dev.tsv\n",
        "  - 73と同じkey_to_idxとembedding_matrixを用意\n",
        "  - devをtoken化してDataset / DataLoaderを作成する"
      ],
      "metadata": {
        "id": "JgihAyfJ9qfM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7nYqXLq7LKD",
        "outputId": "eacd570b-a80c-4af2-8a0a-acab8a7ff7ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Any, Dict, List, Set, Tuple, Union\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 73でsaveしたファイルを読み込む\n",
        "bundle = torch.load(\n",
        "    \"/content/drive/MyDrive/section_73_bundle.pth\",\n",
        "    map_location=device\n",
        ")"
      ],
      "metadata": {
        "id": "Ht44aySEcah3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key_to_idx = bundle[\"key_to_idx\"]\n",
        "# Dataset.__getitem__ は CPU 上で実行されるため、\n",
        "# input_ids は常に CPU Tensor になる。\n",
        "# embedding_matrix が GPU Tensor だと\n",
        "# embedding_matrix[input_ids] で device mismatch が起きるため、\n",
        "# embedding_matrix は明示的に CPU に置いておく。\n",
        "embedding_matrix = bundle[\"embedding_matrix\"].to(\"cpu\")\n",
        "embedding_dim = bundle[\"meta\"][\"embedding_dim\"]\n",
        "threshold = bundle[\"meta\"][\"threshold\"]"
      ],
      "metadata": {
        "id": "tDJhfsPucyit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SSTDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset Class for the SST-2.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data: List[Dict[str, torch.Tensor]], embedding_matrix: torch.Tensor) -> None:\n",
        "        super().__init__() # ほとんど何もしていないらしいが継承クラスを初期化する慣習\n",
        "        self.data = data\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        object = self.data[index]\n",
        "        input_ids = object[\"input_ids\"]\n",
        "        embeddings = self.embedding_matrix[input_ids]\n",
        "\n",
        "        # 平均化ベクトルの取得\n",
        "        # torch.meanはtensorに含まれるすべての要素の平均を返す\n",
        "        # dimを0と指定すると列方向に平均、1と指定すると行方向に平均をとる\n",
        "        mean_embedding = torch.mean(embeddings, dim=0)\n",
        "        return mean_embedding, object[\"label\"]"
      ],
      "metadata": {
        "id": "ZRj3JnV1dYjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SemanticClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Bag of words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dimension: int, n_classes: int) -> None:\n",
        "        super().__init__()\n",
        "        self.in_dimension = in_dimension\n",
        "        self.n_classes = n_classes\n",
        "        self.linear1 = nn.Linear(in_features=in_dimension, out_features=1, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # squeeze(1) は「index が 1 の軸のサイズが 1 のときだけ、その軸を削除して次元を 1 つ減らす」\n",
        "        return self.sigmoid(self.linear1(x)).squeeze(1)"
      ],
      "metadata": {
        "id": "9LIdaaEEcZ-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_seeds(seed: int) -> None:\n",
        "    \"\"\"Fix seeds, Pytorch, random, numpy.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    seed : int\n",
        "        Number of a seed.\n",
        "    \"\"\"\n",
        "    # random.random()・random.shuffle()などに影響する\n",
        "    # 今回は使用されていなそうだが慣習で設定するようにしている\n",
        "    random.seed(seed)\n",
        "\n",
        "    # おそらく間違いで書かれていたコード\n",
        "    # これはseed付き乱数生成器を作っているだけで使っていない（インスタンス化している）\n",
        "    # npのseedは使われていなかったので大丈夫だった\n",
        "    np.random.RandomState(seed)\n",
        "    #正解はグローバルなnumpyの乱数を固定するために定義したこれ\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # PyTorch（CPU）の乱数を固定する\n",
        "    torch.manual_seed(seed)\n",
        "    # GPUの場合\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "    # GPU内で同じ入力の場合に同じ出力を保証する設定\n",
        "    # cuDNNはNVIDIAが提供するGPU用の高速数値計算ライブラリ\n",
        "    # PyTorchでは内部でこれを使用してLinear、Conv、RNNなどを高速化している\n",
        "    # 計算速度を多少遅くするが結果が必ず同じアルゴリズムだけを使用するようにできる\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "tL30TxB_dPth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(row: pd.Series, key_to_idx: Dict[str, int]) -> Tuple[Dict[str, Any], int]:\n",
        "    \"\"\"Convert inputted text and label to dict object.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    row : pd.Series\n",
        "        Row of the dataset.\n",
        "    key_to_idx : Dict[str, int]\n",
        "        Dictionary of word to index.\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[Dict[str, Any], int]\n",
        "        Tokenized data dictionary and token count\n",
        "    \"\"\"\n",
        "    sentence = row[\"sentence\"]\n",
        "    label = row[\"label\"]\n",
        "    input_ids = []\n",
        "\n",
        "    for word in sentence.lower().split():\n",
        "        if word in key_to_idx:\n",
        "            input_ids.append(key_to_idx[word])\n",
        "\n",
        "    token_dict = {\n",
        "        \"text\": sentence,\n",
        "        \"label\": torch.tensor(label, dtype=torch.long),\n",
        "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "    }\n",
        "\n",
        "    return token_dict, len(input_ids)\n"
      ],
      "metadata": {
        "id": "O5eMKCAXePPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_token(df: pd.DataFrame, key_to_idx: Dict[str, int]) -> List[Dict[str, torch.Tensor]]:\n",
        "    \"\"\"Apply tokenize function to each row of the dataframe.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Dataset dataframe.\n",
        "    key_to_idx : Dict[str, int]\n",
        "        Dictionary of word to index.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[Dict[str, torch.Tensor]]\n",
        "        List of tokenized data dictionaries\n",
        "    \"\"\"\n",
        "    # sentenceのindex化\n",
        "    # argsとはapplyで呼ばれる関数にrow以外で追加で渡したい引数を指定するもの\n",
        "    # tupleで指定している理由\n",
        "    # argsはtupleで渡さないといけないので(key_to_idx,)としている:(key_to_idx)だとint\n",
        "    tokenized_data = df.apply(tokenize, args=(key_to_idx,), axis=1)\n",
        "\n",
        "    # token数が0の行を除く\n",
        "    #.token_count = input_idsで単語ベクトルが存在しないものをフィルターしている\n",
        "    result = [token_dict for token_dict, token_count in tokenized_data if token_count > 0]\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "UHQ8ik6feN0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dev(\n",
        "    model: nn.Module,\n",
        "    devloader: DataLoader,\n",
        "    criterion: nn.BCELoss,\n",
        "    device: torch.device,\n",
        "    threshold: float,\n",
        "):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for mean_embedding, label in tqdm(devloader, desc=\"Evaluating\"):\n",
        "            mean_embedding = mean_embedding.to(device)\n",
        "            label = label.to(device).to(torch.float32)\n",
        "\n",
        "            pred = model(mean_embedding)\n",
        "            loss = criterion(pred, label)\n",
        "\n",
        "            pred_binary = (pred >= threshold).float()\n",
        "            correct += (pred_binary == label).sum().item()\n",
        "            total += label.size(0)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
        "    acc = correct / total if total > 0 else 0.0\n",
        "    # set_postfixは進捗バーの横に出力されるが\n",
        "    # writeは新しく行で出力される\n",
        "    tqdm.write(f\"loss so far: {avg_loss:.4f}\")\n",
        "    tqdm.write(f\"acc so far: {acc:.4f}\")\n",
        "    return avg_loss, acc\n"
      ],
      "metadata": {
        "id": "siTF9xNefj0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args) -> None:\n",
        "    fix_seeds(args.seed)\n",
        "    dev_df = pd.read_csv(\"/content/drive/MyDrive/SST-2/dev.tsv\", sep=\"\\t\")\n",
        "    dev_data = convert_to_token(dev_df, key_to_idx)\n",
        "    dev_dataset = SSTDataset(dev_data, embedding_matrix)\n",
        "    dev_loader = DataLoader(dev_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "    model = SemanticClassifier(in_dimension=embedding_matrix.size(1), n_classes=2).to(device)\n",
        "\n",
        "    # 73で学習したモデルをロードする\n",
        "    model.load_state_dict(bundle[\"model_state_dict\"])\n",
        "    criterion = nn.BCELoss()\n",
        "    avg_loss, avg_accuracy = dev(\n",
        "            model=model,\n",
        "            devloader=dev_loader,\n",
        "            criterion=criterion,\n",
        "            device=device,\n",
        "            threshold=threshold,\n",
        "        )\n",
        "    print(f\"final loss: {avg_loss:.4f}\")\n",
        "    print(f\"final accuracy: {avg_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "AXElFT0Ocxrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # pythonスクリプトをコマンドラインから実行するときに渡された引数（args）を定義通りに解釈・解析（parse）するための# 標準ライブラリ\n",
        "    import argparse\n",
        "\n",
        "    # ArgumentParserのインスタンスを作成する（argumentを登録するために必要）\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add_argument(短い名前（省略可）, 長い名前, type・defaultなどの解釈ルール設定)\n",
        "    parser.add_argument(\"-s\", \"--seed\", type=int, default=29)\n",
        "    parser.add_argument(\"-e\", \"--epochs\", default=100, type=int)\n",
        "    parser.add_argument(\"-b\", \"--batch_size\", default=32, type=int)\n",
        "    parser.add_argument(\"-p\", \"--postfix\", type=str) # 今の所使用していない\n",
        "\n",
        "    # store_trueはそのオプションが指定されたらTrue、指定されなかったらFalseにする\n",
        "    parser.add_argument(\"--dryrun\", action=\"store_true\")\n",
        "\n",
        "    # 内部でimport sysをしているのでimport不要\n",
        "    # sys.argvを読んで登録したルールに従って解析をする\n",
        "    # sys.argvの出力例\n",
        "    # ['train.py', '-e', '50', '--dryrun']\n",
        "    # colabでは特有のargsを渡してくるので定義したargsのみ受け取るようにする\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    main(args)"
      ],
      "metadata": {
        "id": "7J3mxU-SefvJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}