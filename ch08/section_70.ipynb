{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTRNGukgb4lS8t+HFxjY5E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haru1489248/nlp-100-nock/blob/main/ch08/section_70.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 単語埋め込みの読み込み\n",
        "事前学習済みの単語ベクトルを読み込んで、行列Eを作成する\n",
        "条件\n",
        "- E[0]には\\<PAD>用にゼロベクトル\n",
        "- E[1:]以降に事前学習済みベクトルを順に格納\n",
        "- 同時にtoken ↔︎ token_idの対応（双方向対応）を保持\n",
        "\n",
        "### 具体的な説明\n",
        "1. 行列Eとはどうなっているか\n",
        "   - Eは「単語ID -> 単語ベクトル」を引ける表\n",
        "   - サイズは（V * d）\n",
        "     - V = 語彙数（トークン数）\n",
        "     - d = 埋め込み次元（1単語あたりのベクトルの長さ）\n",
        "2. PADとは\n",
        "   - 自然言語処理では、文の長さがバラバラなので、見にバッチ処理を追加するために長さを揃える必要がある。\n",
        "   - そのときに埋めるtokenが\\<PAD>（padding token）である。  \n",
        "   - なぜPADをゼロベクトルにするか\n",
        "     1. 計算に影響を与えにくい（無害にしたい）\n",
        "        - \\<PAD>は本当の単語ではなく穴埋めなのでモデルの判断に影響させたくない\n",
        "        - ゼロベクトルなら、後段で足し算や平均を取ったときに邪魔しにくい\n",
        "     2. マスク・無視処理がしやすい\n",
        "        - 多くのライブラリや実装では「padding_idx=0（ID=0はPAD）」みたいに0番を特別扱いにする記法がよくある\n",
        "        - 先頭行をPADに予約すると実装がシンプルになる\n",
        "     3. PADを学習で更新しないようにできる\n",
        "        - PyTorchのnn.Embedding(..., padding_idx=0)は、PAD行をほぼ固定できる。\n",
        "3. 双方向紐付けとは\n",
        "   - token2id: 単語（トークン）-> 行列Eの行番号（ID）\n",
        "   - id2token: 行番号（ID）-> 単語（トークン）"
      ],
      "metadata": {
        "id": "xS4DTk7PGycq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHpDC_PfGwmA",
        "outputId": "86388be0-13e4-420f-c63c-665f5e7d2416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gensim # colabにはデフォルトで入っていないのでinstallする"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVS8VwGINLcy",
        "outputId": "51b9fcbf-f6b1-41c1-aada-902c0ce5e50b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KeyedVectorsとは何をしている？\n",
        "- 学習済みの「単語（キー）→ベクトル」の対応を保持するクラス\n",
        "- 単語を指定すると対応する埋め込みベクトルを取り出せる（参照できる）\n",
        "- 学習機能は持たず、読み込み・参照に特化している\n"
      ],
      "metadata": {
        "id": "9omUxVKYX0fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Z3XZM-0iNdVy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv_src = '/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz' # gzipはデータの中の繰り返しパターンを短い表現に置き換える\n",
        "wv = KeyedVectors.load_word2vec_format(wv_src, binary=True) # binary=Trueでバイナリ形式（0, 1のバイト列）で対応できるようにする"
      ],
      "metadata": {
        "id": "QSYwBT1kNpqg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### len() で語彙数が分かる理由\n",
        "`len(wv)` を呼び出すと、内部では `wv.__len__()` が実行される。\n",
        "\n",
        "gensim の `KeyedVectors` クラスでは、`__len__()` メソッドが\n",
        "内部の `key_to_index`（単語 → インデックスの辞書）の長さを\n",
        "返すように実装されている。\n",
        "\n",
        "```python\n",
        "def __len__(self):\n",
        "    return len(self.key_to_index)\n",
        "```\n",
        "\n",
        "### byteの単位\n",
        "\n",
        "```\n",
        "1 KB = 1024 bytes\n",
        "1 MB = 1024 KB\n",
        "1 GB = 1024 MB\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zp3VicqbeIJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V = len(wv) + 1 # +1は<PAD>分\n",
        "print('語彙数:', V)\n",
        "\n",
        "d = wv.vector_size\n",
        "print('埋め込み次元数', d)\n",
        "\n",
        "print(wv.vectors.shape) # GoogleNews-vectors-negative300の埋め込み行列\n",
        "print(wv.vectors.nbytes / 1024**3) # 使用メモリ数"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3262QXwRcc2-",
        "outputId": "c9ed2861-661c-4808-9522-9ed5f921aaff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "語彙数: 3000001\n",
            "埋め込み次元数 300\n",
            "(3000000, 300)\n",
            "3.3527612686157227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "K8MuXlolf0NE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.zeros()はこの場合、(V * d)の行列を作り、全部ゼロで埋めている\n",
        "dtype=torch.float32は各要素のデータ型でfloat32は32ビット浮動小数点数"
      ],
      "metadata": {
        "id": "dmFVy2rEpcIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "E = torch.zeros((V, d), dtype=torch.float32) # PyTorchのニューラルネットが期待している型がfloat32らしい。今回は学習範囲外\n",
        "token2id = {\"<PAD>\": 0}\n",
        "id2token = [\"<PAD>\"]"
      ],
      "metadata": {
        "id": "wZwgCgwqYrpj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, token in enumerate(wv.index_to_key, start=1): # すでに0のindexはPADに割り当てているので1から開始する\n",
        "  token2id[token] = i\n",
        "  id2token.append(token)\n",
        "  E[i] = torch.from_numpy(wv[token]) # from_numpyはnumpy配列をPyTorchテンソル型に変換する関数（torch.Tensor）"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nZIx7t-c6yY",
        "outputId": "c5346649-153a-4ae0-eddb-dc8107912d89"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-869294723.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "  E[i] = torch.from_numpy(wv[token]) # from_numpyはnumpy配列をPyTorchテンソル型に変換する関数（torch.Tensor）\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Embedding matrix info ===\")\n",
        "print(\"E shape:\", E.shape)           # (V, d) になっているか\n",
        "print(\"Embedding dim (d):\", E.shape[1])\n",
        "print(\"Vocab size (V):\", E.shape[0])\n",
        "print(f\"idx2token[1]:{id2token[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-NIcoSjrf8a",
        "outputId": "de266b51-0896-4daa-e6f2-ef9929394661"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Embedding matrix info ===\n",
            "E shape: torch.Size([3000001, 300])\n",
            "Embedding dim (d): 300\n",
            "Vocab size (V): 3000001\n",
            "idx2token[1]:</s>\n"
          ]
        }
      ]
    }
  ]
}