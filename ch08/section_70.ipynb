{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpdNZzOg0gFeqcHoAstL4P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haru1489248/nlp-100-nock/blob/main/ch08/section_70.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 単語埋め込みの読み込み\n",
        "事前学習済みの単語ベクトルを読み込んで、行列Eを作成する\n",
        "条件\n",
        "- E[0]には\\<PAD>用にゼロベクトル\n",
        "- E[1:]以降に事前学習済みベクトルを順に格納\n",
        "- 同時にtoken ↔︎ token_idの対応（双方向対応）を保持\n",
        "\n",
        "### 具体的な説明\n",
        "1. 行列Eとはどうなっているか\n",
        "   - Eは「単語ID -> 単語ベクトル」を引ける表\n",
        "   - サイズは（V * d）\n",
        "     - V = 語彙数（トークン数）\n",
        "     - d = 埋め込み次元（1単語あたりのベクトルの長さ）\n",
        "2. PADとは\n",
        "   - 自然言語処理では、文の長さがバラバラなので、見にバッチ処理を追加するために長さを揃える必要がある。\n",
        "   - そのときに埋めるtokenが\\<PAD>（padding token）である。  \n",
        "   - なぜPADをゼロベクトルにするか\n",
        "     1. 計算に影響を与えにくい（無害にしたい）\n",
        "        - \\<PAD>は本当の単語ではなく穴埋めなのでモデルの判断に影響させたくない\n",
        "        - ゼロベクトルなら、後段で足し算や平均を取ったときに邪魔しにくい\n",
        "     2. マスク・無視処理がしやすい\n",
        "        - 多くのライブラリや実装では「padding_idx=0（ID=0はPAD）」みたいに0番を特別扱いにする記法がよくある\n",
        "        - 先頭行をPADに予約すると実装がシンプルになる\n",
        "     3. PADを学習で更新しないようにできる\n",
        "        - PyTorchのnn.Embedding(..., padding_idx=0)は、PAD行をほぼ固定できる。\n",
        "3. 双方向紐付けとは\n",
        "   - token2id: 単語（トークン）-> 行列Eの行番号（ID）\n",
        "   - id2token: 行番号（ID）-> 単語（トークン）"
      ],
      "metadata": {
        "id": "xS4DTk7PGycq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHpDC_PfGwmA",
        "outputId": "236718ce-f7b3-4095-aeec-bd6803200b7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gensim # colabにはデフォルトで入っていないのでinstallする"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVS8VwGINLcy",
        "outputId": "e17c1cd8-0070-4b6e-cab9-5ba0312e927f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KeyedVectorsとは何をしている？\n",
        "- 学習済みの「単語（キー）→ベクトル」の対応を保持するクラス\n",
        "- 単語を指定すると対応する埋め込みベクトルを取り出せる（参照できる）\n",
        "- 学習機能は持たず、読み込み・参照に特化している\n"
      ],
      "metadata": {
        "id": "9omUxVKYX0fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Z3XZM-0iNdVy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv_src = '/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz' # gzipはデータの中の繰り返しパターンを短い表現に置き換える\n",
        "wv = KeyedVectors.load_word2vec_format(wv_src, binary=True) # binary=Trueでバイナリ形式（0, 1のバイト列）で対応できるようにする"
      ],
      "metadata": {
        "id": "QSYwBT1kNpqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### len() で語彙数が分かる理由\n",
        "`len(wv)` を呼び出すと、内部では `wv.__len__()` が実行される。\n",
        "\n",
        "gensim の `KeyedVectors` クラスでは、`__len__()` メソッドが\n",
        "内部の `key_to_index`（単語 → インデックスの辞書）の長さを\n",
        "返すように実装されている。\n",
        "\n",
        "```python\n",
        "def __len__(self):\n",
        "    return len(self.key_to_index)\n",
        "```\n",
        "\n",
        "### byteの単位\n",
        "\n",
        "```\n",
        "1 KB = 1024 bytes\n",
        "1 MB = 1024 KB\n",
        "1 GB = 1024 MB\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zp3VicqbeIJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V = len(wv) + 1 # +1は<PAD>分\n",
        "print('語彙数:', V)\n",
        "\n",
        "d = wv.vector_size\n",
        "print('埋め込み次元数', d)\n",
        "\n",
        "print(wv.vectors.shape) # GoogleNews-vectors-negative300の埋め込み行列\n",
        "print(wv.vectors.nbytes / 1024**3) # 使用メモリ数"
      ],
      "metadata": {
        "id": "3262QXwRcc2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "K8MuXlolf0NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.zeros()はこの場合、(V * d)の行列を作り、全部ゼロで埋めている\n",
        "dtype=torch.float32は各要素のデータ型でfloat32は32ビット浮動小数点数"
      ],
      "metadata": {
        "id": "dmFVy2rEpcIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "E = torch.zeros((V, d), dtype=torch.float32) # PyTorchのニューラルネットが期待している型がfloat32らしい。今回は学習範囲外\n",
        "token2id = {\"<PAD>\": 0}\n",
        "id2token = [\"<PAD>\"]"
      ],
      "metadata": {
        "id": "wZwgCgwqYrpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### torch.tensorとtorch.from_numpyの違い\n",
        "- `torch.tensor(...)`\n",
        "  - tensor型に変換する\n",
        "  - 基本的にメモリを新しく確保（コピーする）\n",
        "- `torch.from_numpy(...)`\n",
        "  - Numpy配列 -> tensor型に変換する\n",
        "  - メモリを複製しない（Numpy配列とメモリ共有）"
      ],
      "metadata": {
        "id": "0nhIsUhLm89i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, token in enumerate(wv.index_to_key, start=1): # すでに0のindexはPADに割り当てているので1から開始する\n",
        "  token2id[token] = i\n",
        "  id2token.append(token)\n",
        "  E[i] = torch.tensor(wv[token]) # from_numpyはnumpy配列をPyTorchテンソル型に変換する関数（torch.Tensor）"
      ],
      "metadata": {
        "id": "1nZIx7t-c6yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Embedding matrix info ===\")\n",
        "print(\"E shape:\", E.shape)           # (V, d) になっているか\n",
        "print(\"Embedding dim (d):\", E.shape[1])\n",
        "print(\"Vocab size (V):\", E.shape[0])\n",
        "print(f\"idx2token[1]:{id2token[1]}\")"
      ],
      "metadata": {
        "id": "2-NIcoSjrf8a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}