{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMQZMuzoN1FWOxx5bQq9VRX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haru1489248/nlp-100-nock/blob/main/ch10/section_91.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 91. 続きのテキスト予測\n",
        "“The movie was full of”に続くテキストを複数予測せよ。このとき、デコーディングの方法や温度パラメータ（temperature）を変えながら、予測される複数のテキストの変化を観察せよ。\n",
        "### temperature（温度）パラメータとは\n",
        "- softmax関数に渡す定数（例としてTとしている）\n",
        "$$\n",
        "p_i = softmax(\\frac{z_i}{T})\n",
        "$$\n",
        "### 温度パラメータの変更による影響\n",
        "- 温度パラメータはGPTが応答を生成する際に、生成される応答の多様性を制御するために調整される。\n",
        "- 温度パラメータが高いほど、よりランダムな応答が生成される。\n",
        "- 逆に温度パラメータが低いほど、より確信度の高い応答が生成される。\n",
        "\n",
        "補足\n",
        "- 今回はPADとEOSを設定する必要はないが慣習として設定する"
      ],
      "metadata": {
        "id": "57Sng2ShiJJY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTkZYn-ziDk0",
        "outputId": "8c8748bd-ca95-40bc-8e8e-5b3268332db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GenerateConfigとは？\n",
        "- transformersライブラリに用意されているテキスト生成専用の設定クラス\n",
        "- model.generate()の引数に設定して、デコーディング時の挙動をまとめて指定するためのクラス\n"
      ],
      "metadata": {
        "id": "McbkLjnTYILE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig"
      ],
      "metadata": {
        "id": "XXHjEpBLiIfu"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"gpt2\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "temperatures = [1.0, 0.7, 0.5, 0.1]\n",
        "max_new_tokens = 50"
      ],
      "metadata": {
        "id": "7guDYCcNiXcw"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "wEvcFEbkcMxk"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "9OFOpdgetQpO"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)"
      ],
      "metadata": {
        "id": "TyM7_ZOEiwN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The movie was full of\"\n",
        "\n",
        "# paddingとtruncationは慣習的に設定している\n",
        "input_ids = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)"
      ],
      "metadata": {
        "id": "wghbSc3Hia4O"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.generate()はtoken_idのtensorを返す\n",
        "## Greedy methodとは？\n",
        "- 毎ステップ一番確率が高い次トークンを一つだけ選び続けるデコーディング方法"
      ],
      "metadata": {
        "id": "PtYpR5mVdBad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Greedy method\")\n",
        "with torch.no_grad():\n",
        "  generation_config = GenerationConfig(\n",
        "      max_new_tokens=max_new_tokens,\n",
        "      pad_token_id=tokenizer.pad_token_id,\n",
        "      eos_token_id=tokenizer.eos_token_id,\n",
        "      do_sample=False, # beamが指定されていない場合はGreedy\n",
        "  )\n",
        "  output = model.generate(**input_ids, generation_config=generation_config)\n",
        "  sampling_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "  print(sampling_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liLLEekXjr1U",
        "outputId": "5dd7bd30-c4b2-4b28-c7c0-47b4dcb155ea"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy method\n",
            "The movie was full of jokes and jokes about how the movie was a joke. It was a joke about how the movie was a joke. It was a joke about how the movie was a joke. It was a joke about how the movie was a joke. It was a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BeamSearch（ビームサーチ）とは？\n",
        "- 生成は1トークンずつ進む。各ステップで候補（ビーム）を展開し、累積スコア（通常はlog確率の和）が高い上位 beam_size 本だけを残す（剪定する）。\n",
        "- これを max_new_tokens 回（または EOS が出るまで）繰り返し、最終的に最もスコアの高い系列（ルート）を出力する。\n"
      ],
      "metadata": {
        "id": "mqWh-xtJjkhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"BeamSearch（num_beams = 5）\")\n",
        "with torch.no_grad():\n",
        "  generation_config = GenerationConfig(\n",
        "      max_new_tokens=max_new_tokens,\n",
        "      pad_token_id=tokenizer.pad_token_id,\n",
        "      eos_token_id=tokenizer.eos_token_id,\n",
        "      do_sample=False,\n",
        "      num_beams=5\n",
        "  )\n",
        "  output = model.generate(**input_ids, generation_config=generation_config)\n",
        "  sampling_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "  print(sampling_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sRQMbEfgtuE",
        "outputId": "4efa4d97-4596-40e6-ea9e-f0be153113c3"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BeamSearch（num_beams = 5）\n",
            "The movie was full of jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and jokes and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Multinominal Sampling\")\n",
        "for t in temperatures:\n",
        "  print(f\"Tempreture = {t}\")\n",
        "  with torch.no_grad():\n",
        "    generation_config = GenerationConfig(\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        num_beams=1,\n",
        "        temperature=t,\n",
        "    )\n",
        "    output = model.generate(**input_ids, generation_config=generation_config)\n",
        "    sampling_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(sampling_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNqTB_OGf1Yz",
        "outputId": "43eb41b0-0de8-4c95-a63c-2e3616b8a46e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multinominal Sampling\n",
            "Tempreture = 1.0\n",
            "The movie was full of jokes about how the show is \"totally not real.\" The cast was also shocked with it, and it certainly wasn't just about the kids growing up in suburban Detroit.\n",
            "\n",
            "On the plus side, James and his co-workers were making\n",
            "Tempreture = 0.7\n",
            "The movie was full of references to the 1970s, but it was also full of references to how the world of the film would react in the years to come. It's one of those movies that's full of great ideas, but I can't think of one in the\n",
            "Tempreture = 0.5\n",
            "The movie was full of great action. The film is about a group of young girls who are given a choice between a life of crime and a life of love. The film is about a group of young girls who are given a choice between a life of crime and a life\n",
            "Tempreture = 0.1\n",
            "The movie was full of great moments, but it was also full of bad moments.\n",
            "\n",
            "I'm not sure if it's because I'm a fan of the movie or if it's because I'm a fan of the characters. I'm not sure if it's because\n"
          ]
        }
      ]
    }
  ]
}