{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNAGLCRn4O5IMLjvYBVZaxY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haru1489248/nlp-100-nock/blob/main/ch10/section_90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 90. 次単語予測\n",
        "“The movie was full of”に続くトークン（トークン列ではなく一つのトークンであることに注意せよ）として適切なもの上位10個と、その確率（尤度）を求めよ。ただし、言語モデルへのプロンプトがどのようなトークン列に変換されたか、確認せよ。"
      ],
      "metadata": {
        "id": "lmvRJLlMFryp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAbpPExx6PiS",
        "outputId": "8da4a8d3-12f2-45a7-d8c1-0313f89032f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "z5XjVSLKGZT0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'gpt2'"
      ],
      "metadata": {
        "id": "BQfyvzy4I0ps"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "wN63sIsUGj4a"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "7V66N3CGGprx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The movie was full of\"\n",
        "\n",
        "encodings = tokenizer(text, return_tensors='pt')"
      ],
      "metadata": {
        "id": "Wtvqal95Eawi"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = encodings['input_ids'].squeeze(0)\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(f\"tokens: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28MnN-67HJ74",
        "outputId": "fae3fdad-3247-4adc-a2e0-00a0141e5b17"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens: ['The', 'Ġmovie', 'Ġwas', 'Ġfull', 'Ġof']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  outputs = model(**encodings) # **はkeyword引数を展開するpythonの記法"
      ],
      "metadata": {
        "id": "hVxH8CUmHP5-"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`outputs.logits[0, -1]`とは何か？\n",
        "- logitsのshapeは(batch_size, seq_len, vocab_size)となっている\n",
        "- `[0, -1]`は省略形で、実際は`[0, -1, :]`と同じである\n",
        "- よってbatch_sizeの0番目、seq_lenの-1番目、vocab_sizeは全てとなる\n",
        "  - batch_sizeの0番目: 今回は1つしかないため変わらない\n",
        "  - seq_lenの-1番目: 最後のトークン\n",
        "なぜ最後のトークンなのか\n",
        "- GPTは各位置tで次の単語の予測をしている\n",
        "- 今回は文章に続く単語を予測しないといけないので文章の中で最後の文字の次の単語の予測を取得しないといけない"
      ],
      "metadata": {
        "id": "_OtsWFE8WXhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = outputs.logits[0, -1]\n",
        "print(f\"last logits: {logits}\")\n",
        "print(f\"All logits: {outputs.logits}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wUhaorjICSp",
        "outputId": "faa281aa-daf4-4da2-aa7d-4c2b518e37ed"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "last logits: tensor([-102.3519, -101.2763, -106.8403,  ..., -103.5513, -107.0933,\n",
            "        -102.1604])\n",
            "All logits: tensor([[[ -36.2874,  -35.0114,  -38.0793,  ...,  -40.5163,  -41.3760,\n",
            "           -34.9193],\n",
            "         [ -91.6833,  -91.4114,  -94.6155,  ..., -100.2464,  -97.9104,\n",
            "           -92.4564],\n",
            "         [-125.3571, -124.7983, -127.3332,  ..., -134.9384, -128.8435,\n",
            "          -126.3980],\n",
            "         [ -52.1309,  -54.5084,  -62.0278,  ...,  -62.6903,  -61.0079,\n",
            "           -54.3363],\n",
            "         [-102.3519, -101.2763, -106.8403,  ..., -103.5513, -107.0933,\n",
            "          -102.1604]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "語彙サイズの尤度（確率）が手に入るので、各インデックスがそのままconvert_ids_to_tokensに渡せる"
      ],
      "metadata": {
        "id": "qyw_WX1MY8WI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prods = torch.softmax(logits, dim=-1) # logits.shape = (vocab_size,)\n",
        "print(f\"prods: {prods}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjD5F-4KIDLg",
        "outputId": "db580bad-4060-48b3-f458-62bd68476fd7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prods: tensor([2.5071e-06, 7.3499e-06, 2.8175e-08,  ..., 7.5550e-07, 2.1876e-08,\n",
            "        3.0362e-06])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "topkは上位k個の要素と、その要素のindexを返す"
      ],
      "metadata": {
        "id": "LSW9yPvlZRus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topk = torch.topk(prods, k=10)\n",
        "print(f\"topk: {topk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVfyG8OHVwWq",
        "outputId": "0d36968a-d804-43fc-b291-17b9b9ac4367"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topk: torch.return_types.topk(\n",
            "values=tensor([0.0219, 0.0186, 0.0115, 0.0109, 0.0107, 0.0105, 0.0100, 0.0074, 0.0074,\n",
            "        0.0067]),\n",
            "indices=tensor([14532,  1049, 22051,  2089, 24072, 10288,  1257, 14733,   366,   262]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ":>12 とは？\n",
        "- Python の f-string の書式指定\n",
        "  - : 書式指定を始める\n",
        "  - > 右寄せ\n",
        "  - 12 幅 12 文字\n",
        "\n",
        "!r とは？\n",
        "- repr() を使って表示する指定\n",
        "- 先頭の空白や改行など、人間が通常見落とす文字も確認できる\n",
        "\n",
        ".6f とは？\n",
        "- 小数点以下 6 桁まで表示（四捨五入）\n",
        "\n",
        "tokenizer.decode はなぜ tokenizer.decode([idx.item()]) とするのか？\n",
        "- tokenizer.decode は「トークン列（list[int]）」から文字列に変換する関数\n",
        "- 1トークンでも list に包む必要がある\n",
        "- idx は Tensor なので、まず item() で Python の int に変換する\n",
        "\n",
        "item() とは？\n",
        "- torch.Tensor（1要素）を Python のスカラー型（int / float）に変換するメソッド\n"
      ],
      "metadata": {
        "id": "Q3G8jEDSZw9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTop-10 next tokens:\")\n",
        "for p, idx in zip(topk.values, topk.indices):\n",
        "  token_str = tokenizer.convert_ids_to_tokens(idx.item()) # item()でpython互換に変換する\n",
        "  decoded = tokenizer.decode([idx.item()])\n",
        "  print(f\"{token_str:>12} | {decoded!r:>12} | {p.item():.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHn1Iw5_V1XM",
        "outputId": "d3f5ecc2-a920-43b3-9743-149eb103cbd1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-10 next tokens:\n",
            "      Ġjokes |     ' jokes' | 0.021892\n",
            "      Ġgreat |     ' great' | 0.018644\n",
            "     Ġlaughs |    ' laughs' | 0.011524\n",
            "        Ġbad |       ' bad' | 0.010874\n",
            "  Ġsurprises | ' surprises' | 0.010667\n",
            " Ġreferences | ' references' | 0.010528\n",
            "        Ġfun |       ' fun' | 0.009992\n",
            "      Ġhumor |     ' humor' | 0.007415\n",
            "          Ġ\" |         ' \"' | 0.007408\n",
            "        Ġthe |       ' the' | 0.006709\n"
          ]
        }
      ]
    }
  ]
}