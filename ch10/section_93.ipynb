{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMJybh016zSgq3cT/BXMwW2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haru1489248/nlp-100-nock/blob/main/ch10/section_93.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 93. パープレキシティ\n",
        "適当な文を準備して、事前学習済み言語モデルでパープレキシティを測定せよ。例えば、\n",
        "\n",
        "The movie was full of surprises\n",
        "\n",
        "The movies were full of surprises\n",
        "\n",
        "The movie were full of surprises\n",
        "\n",
        "The movies was full of surprises\n",
        "\n",
        "の4文に対して、パープレキシティを測定して観察せよ（最後の2つの文は故意に文法的な間違いを入れた）。\n",
        "\n",
        "### パープレキシティ（Perplexity）\n",
        "\n",
        "パープレキシティ（Perplexity）は、\n",
        "言語モデルが次の単語をどれだけ予測しにくいかを表す評価指標である。\n",
        "\n",
        "- パープレキシティが 低い 場合\n",
        "→ モデルは次の単語を高い確率で予測できている\n",
        "\n",
        "- パープレキシティが 高い 場合\n",
        "→ 次の単語の候補が多く、予測が難しい\n",
        "\n",
        "直感的には、\n",
        "「次の単語が平均して何択に見えているか」\n",
        "を表す指標と解釈できる。\n",
        "\n",
        "### 正解率が適さない理由\n",
        "\n",
        "言語モデルの予測では、次に来る単語が必ずしも一意に定まらない。\n",
        "そのため、正解・不正解のみで評価する正解率（accuracy）は、\n",
        "言語モデルの性能評価には適していない。\n",
        "\n",
        "パープレキシティは、\n",
        "正解単語に割り当てられた確率の大きさを用いて評価を行うため、\n",
        "言語モデルに適した指標である。\n",
        "\n",
        "### エントロピーとの関係\n",
        "\n",
        "言語モデルは、各単語位置において\n",
        "「正解単語が出現する確率」を出力する。\n",
        "\n",
        "エントロピー\n",
        "$$H(X)$$は、\n",
        "\n",
        "正解単語の確率に対して\n",
        "\n",
        "対数を取り、マイナスを付けた値を\n",
        "\n",
        "全単語で平均したもの\n",
        "\n",
        "として定義される。\n",
        "$$\n",
        "H(X)\n",
        "=\n",
        "-\\frac{1}{N}\n",
        "\\sum_{i=1}^{N}\n",
        "\\log P(w_i)\n",
        "$$\n",
        "この値が大きいほど、モデルは予測に失敗しているといえる。\n",
        "\n",
        "パープレキシティの定義\n",
        "\n",
        "エントロピーを人間に読みやすい尺度にするために指数関数で変換したものがパープレキシティである。\n",
        "\n",
        "$$\n",
        "\\text{Perplexity}(X) = 2^{H(X)}\n",
        "$$\n",
        "\n",
        "\n",
        "エントロピーが 0 の場合、\n",
        "モデルは常に正解を予測できており、パープレキシティは 1 となる。\n",
        "\n",
        "まとめ\n",
        "\n",
        "エントロピー\n",
        "→ 正解単語に対する予測誤差（損失）の平均\n",
        "\n",
        "パープレキシティ\n",
        "→ それを「次の単語が何択に見えているか」に変換した指標\n",
        "\n",
        "パープレキシティが低いほど、言語モデルの性能は高い"
      ],
      "metadata": {
        "id": "fOL_ZPfm3flQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVm20CEZ3c6s",
        "outputId": "8d957078-9646-4b94-9580-9a96167294fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
      ],
      "metadata": {
        "id": "I6vQcR7J3pO0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"gpt2\"\n",
        "sentences = [\n",
        "    \"The movie was full of surprises\",\n",
        "    \"The movies were full of surprises\",\n",
        "    \"The movie were full of surprises\",\n",
        "    \"The movies was full of surprises\"\n",
        "]"
      ],
      "metadata": {
        "id": "7lCEKMSYSPjs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "M4nlQNJBVs0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.pad_token_id is None:\n",
        "  tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "Ift9_dXLV6pA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "-SsJDFYhVxEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)"
      ],
      "metadata": {
        "id": "ajpGVyPVV59q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 10\n",
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "RSYYSgGhWNsP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### nn.CrossEntropyLossのreducionについて\n",
        "- reuctionには３種類設定できるものがある\n",
        "  1. mean（デフォルト）\n",
        "    - 全要素の損失を平均する\n",
        "    - 返り値はスカラー\n",
        "  2. sum\n",
        "    - 全要素の損失を合計\n",
        "    - 返り値はスカラー\n",
        "  3. none\n",
        "    - 何もまとめない\n",
        "    - 各要素の損失をそのまま返す\n",
        "\n",
        "今回はわかりやすいように何もしない様に意図的に設定した   "
      ],
      "metadata": {
        "id": "swI8qK_EXLFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction=\"none\")"
      ],
      "metadata": {
        "id": "b1WuO8uKWiYK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  outputs = model(**inputs, generation_config=generation_config)\n",
        "logits = outputs.logits"
      ],
      "metadata": {
        "id": "RzyuqKJLX0iD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "shift_input_idsはなぜ最初の1トークンを除外しているのか\n",
        "- [i, 1:]でi番目のテキストを取得して、最初のトークンを飛ばして取得している。\n",
        "- これは最初のトークンは文脈を持たず、次単語予測という言語モデルの枠組みでは正解ラベルを定義できないため、パープレキシティ計算から除外される。\n",
        "shift_logitsはなぜ最後の1トークンを除外しているのか\n",
        "- logitsは各トークン位置において、次のトークンの予測結果を表している。\n",
        "- 最後のトークン位置では次に予測すべきトークンが存在しない。"
      ],
      "metadata": {
        "id": "SErpsEMuY23G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, text in enumerate(sentences):\n",
        "  # contiguous()の必要性\n",
        "  # https://openillumi.com/pytorch-memory-efficiency-contiguous-method-guide/\n",
        "  shift_input_ids = inputs.input_ids[i, 1:].contiguous()\n",
        "  shift_logits = logits[i, :-1, :].contiguous()\n",
        "\n",
        "  # 正解トークンのtoken_idと書くトークン位置のlogitsを渡し、-logP（正解）を計算する\n",
        "  H_i = cross_entropy_loss(shift_logits, shift_input_ids)\n",
        "\n",
        "  # 文内で平均してクロスエントロピーにする\n",
        "  H = H_i.mean()\n",
        "\n",
        "  # 自然対数ベースなので、expでPPLに変換する\n",
        "  ppl = math.e**H\n",
        "\n",
        "  print(f\"\"\"Text: {text}\\nPPL: {ppl:.4f}\\n\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORYikSy8X7A6",
        "outputId": "50e69317-189f-4a23-9a6f-8941decf4729"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: The movie was full of surprises\n",
            "PPL: 99.3539\n",
            "\n",
            "Text: The movies were full of surprises\n",
            "PPL: 126.4828\n",
            "\n",
            "Text: The movie were full of surprises\n",
            "PPL: 278.8822\n",
            "\n",
            "Text: The movies was full of surprises\n",
            "PPL: 274.6658\n",
            "\n"
          ]
        }
      ]
    }
  ]
}