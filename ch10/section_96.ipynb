{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMg0sqSXHbQfxKrupjbI7i3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haru1489248/nlp-100-nock/blob/main/ch10/section_96.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 96. プロンプトによる感情分析\n",
        "事前学習済み言語モデルで感情分析を行いたい。テキストを含むプロンプトを事前学習済み言語モデルに与え、（ファインチューニングは行わずに）テキストのポジネガを予測するという戦略で、SST-2の開発データにおける正解率を測定せよ。\n",
        "\n"
      ],
      "metadata": {
        "id": "6EyXnqIugkjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "kj2N_NZPouKl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXgCSUthfxn8",
        "outputId": "f0d02f1d-533b-4356-8d81-9a55fbb25838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Downloading transformers-5.1.0-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 5.0.0\n",
            "    Uninstalling transformers-5.0.0:\n",
            "      Successfully uninstalled transformers-5.0.0\n",
            "Successfully installed transformers-5.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY-DU4ztgj-z",
        "outputId": "5ace8790-5a16-4cb5-a330-e4bad1e976c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "max_new_tokens = 10\n",
        "batch_size = 32 if torch.cuda.is_available() else 1\n",
        "src_path = \"/content/drive/MyDrive/SST-2/dev.tsv\""
      ],
      "metadata": {
        "id": "s9gGruvCg_27"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### device_mapとは？\n",
        "モデルの重みをどこで動かすかを指定する仕組み。自動でcudaかcpuを使用するかを決めてくれる\n",
        "\n"
      ],
      "metadata": {
        "id": "GS6UaoyjncPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "\n",
        "if tokenizer.pad_token_id is None:\n",
        "  tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "Tga9CMUInBFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = pd.read_csv(src_path, sep=\"\\t\")"
      ],
      "metadata": {
        "id": "qDKtzBO9oscQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig(\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    do_sample=False,\n",
        "    max_new_tokens=max_new_tokens\n",
        ")"
      ],
      "metadata": {
        "id": "RVBVVE3gpWut"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = dev_df[\"sentence\"].tolist()\n",
        "labels = dev_df[\"label\"].tolist()"
      ],
      "metadata": {
        "id": "qkbvmlGfqG_u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"len sentences:\", len(sentences))\n",
        "print(\"len labels:\", len(labels))\n",
        "print(\"len dev df:\", len(dev_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwaztwSVy-w9",
        "outputId": "281d057f-c3c3-4341-b9a4-3446a7bd3710"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len sentences: 872\n",
            "len labels: 872\n",
            "len dev df: 872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "for i in tqdm(range(0, len(sentences), batch_size), total=len(sentences) / batch_size):\n",
        "  batch_sentences = sentences[i:i+batch_size]\n",
        "  batch_labels = labels[i:i+batch_size]\n",
        "\n",
        "  batch_chat_template = []\n",
        "  for s, label in zip(batch_sentences, batch_labels):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\"\"\n",
        "            You are a classification model for the sentiment analyzer.\n",
        "            Answer with exactly one word: positive or negative.\n",
        "            Do not output anything else.\n",
        "            For example, the positive sentence 'The movie was full of fan.' is inputted, you should return positive.\n",
        "            \"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": s\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    chat_template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    batch_chat_template.append(chat_template)\n",
        "\n",
        "  tokenized_batch = tokenizer(\n",
        "      batch_chat_template,\n",
        "      padding=True,\n",
        "      padding_side=\"left\",\n",
        "      return_tensors=\"pt\"\n",
        "  )\n",
        "  batch_input = tokenized_batch[\"input_ids\"].to(device)\n",
        "  input_len = len(batch_input[0])\n",
        "  batch_attention_mask = tokenized_batch[\"attention_mask\"].to(device)\n",
        "  batch_labels = torch.Tensor(batch_labels).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids=batch_input,\n",
        "        attention_mask=batch_attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "  for output, true_label in zip(outputs, batch_labels):\n",
        "    response = tokenizer.decode(output[input_len:], skip_special_tokens=True)\n",
        "    if \"positive\" in response:\n",
        "      pred_label = 1\n",
        "    elif \"negative\" in response:\n",
        "      pred_label = 0\n",
        "    else:\n",
        "      pred_label = -1\n",
        "\n",
        "    if pred_label == true_label:\n",
        "      correct += 1\n",
        "\n",
        "  accuracy = correct / len(sentences)\n",
        "\n",
        "  print(f\"Accuracy: {accuracy:.4f} ({correct} / {len(sentences)})\")"
      ],
      "metadata": {
        "id": "HqfPKk2Cqf5l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}