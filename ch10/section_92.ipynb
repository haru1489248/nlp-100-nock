{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxcZudLjMuZaZx9M1szGr9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haru1489248/nlp-100-nock/blob/main/ch10/section_92.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 91. 続きのテキスト予測\n",
        "“The movie was full of”に続くテキストを複数予測せよ。このとき、デコーディングの方法や温度パラメータ（temperature）を変えながら、予測される複数のテキストの変化を観察せよ。\n",
        "### temperature（温度）パラメータとは\n",
        "- softmax関数に渡す定数（例としてTとしている）\n",
        "$$\n",
        "p_i = softmax(\\frac{z_i}{T})\n",
        "$$\n",
        "### 温度パラメータの変更による影響\n",
        "- 温度パラメータはGPTが応答を生成する際に、生成される応答の多様性を制御するために調整される。\n",
        "- 温度パラメータが高いほど、よりランダムな応答が生成される。\n",
        "- 逆に温度パラメータが低いほど、より確信度の高い応答が生成される。\n",
        "\n",
        "補足\n",
        "- 今回はPADとEOSを設定する必要はないが慣習として設定する"
      ],
      "metadata": {
        "id": "57Sng2ShiJJY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTkZYn-ziDk0",
        "outputId": "f1eef1fa-882b-4521-bbc3-eb84fd8e0734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GenerateConfigとは？\n",
        "- transformersライブラリに用意されているテキスト生成専用の設定クラス\n",
        "- model.generate()の引数に設定して、デコーディング時の挙動をまとめて指定するためのクラス\n"
      ],
      "metadata": {
        "id": "McbkLjnTYILE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig"
      ],
      "metadata": {
        "id": "XXHjEpBLiIfu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"gpt2\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "temperatures = [1.0, 0.7, 0.5, 0.1]\n",
        "max_new_tokens = 50"
      ],
      "metadata": {
        "id": "7guDYCcNiXcw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "wEvcFEbkcMxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "9OFOpdgetQpO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)"
      ],
      "metadata": {
        "id": "TyM7_ZOEiwN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The movie was full of\"\n",
        "\n",
        "# paddingとtruncationは慣習的に設定している\n",
        "input_ids = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)"
      ],
      "metadata": {
        "id": "wghbSc3Hia4O"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step_logitsとは？\n",
        "- iステップ目の全語彙に対するlogits（softmax前の生スコア）\n",
        "\n",
        "### softmaxだけだと？\n",
        "- 各トークンの確率 \\(p_i\\) を得られる\n",
        "- ただし、確率は非常に小さくなりやすく、数値的に不安定になりやすい\n",
        "\n",
        "### log_softmaxとは？\n",
        "- softmax の結果を log にしたもの（= 各トークンの対数確率）\n",
        "- 数式的には  \n",
        "  $$\n",
        "  \\log p_i = \\log(\\frac{e^{z_i}}{\\sum_j e^{z_j}})\n",
        "           = z_i - \\log(\\sum_j e^{z_j})\n",
        "  $$\n",
        "- $$\\log(\\sum_j e^{z_j})$$ は全クラス共通の正規化項（log-sum-exp）であり、各トークンに同じ値を引くだけなので順位（argmax）は変わらない\n",
        "- 実装では最大値を引くなどの工夫により、exp のオーバーフローや確率のアンダーフローを防ぎ、数値的に安定に計算できる（要するにlong型などの限界値に到達するのを防げる）\n",
        "- 対数確率を用いることで、文章全体の尤度を「確率の積」ではなく「対数確率の和」として扱える\n",
        "\n",
        "### scoresはどんな順番で入っているか？\n",
        "- 生成された文章の順序そのまま\n",
        "- scores[i] は「i番目に生成されたトークン」に対応する全語彙の logits\n",
        "- ただし、プロンプトとして入力した文章は含まれない\n",
        "\n",
        "### log_prodsについて\n",
        "- log_prodsは語彙数分の確率が返される\n",
        "- token_idでそれを指定することによって対象の確率を取得できる"
      ],
      "metadata": {
        "id": "PtYpR5mVdBad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Greedy method\")\n",
        "with torch.no_grad():\n",
        "  generation_config = GenerationConfig(\n",
        "      max_new_tokens=max_new_tokens,\n",
        "      pad_token_id=tokenizer.pad_token_id,\n",
        "      eos_token_id=tokenizer.eos_token_id,\n",
        "      do_sample=False, # beamが指定されていない場合はGreedy\n",
        "      return_dict_in_generate=True,\n",
        "      output_scores=True\n",
        "  )\n",
        "  output = model.generate(**input_ids, generation_config=generation_config)\n",
        "  sequences = output.sequences[0]\n",
        "  scores = output.scores\n",
        "\n",
        "  prompt_len = input_ids[\"input_ids\"].shape[1]\n",
        "  new_token_ids = sequences[prompt_len:] # 生成した部分だけ取得\n",
        "\n",
        "  for i, token_id in enumerate(new_token_ids):\n",
        "    step_logits = scores[i][0]\n",
        "    log_prods = torch.log_softmax(step_logits, dim=-1)\n",
        "    logp = log_prods[token_id].item()\n",
        "    p = torch.exp(log_prods[token_id]).item() # 対数で取得した確率を元の確率に戻す\n",
        "\n",
        "    token_str = tokenizer.decode([token_id]).replace(\"\\n\", \"\\\\n\")\n",
        "    print(f\"{i+1:02d} token={token_str!r:12s} p={p:.6f} logp={logp:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liLLEekXjr1U",
        "outputId": "c3a163b5-338a-4426-f7b1-ac6eebe0e672"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy method\n",
            "01 token=' jokes'     p=0.021892 logp=-3.821651\n",
            "02 token=' and'       p=0.289225 logp=-1.240552\n",
            "03 token=' jokes'     p=0.098500 logp=-2.317703\n",
            "04 token=' about'     p=0.205553 logp=-1.582050\n",
            "05 token=' how'       p=0.099715 logp=-2.305439\n",
            "06 token=' the'       p=0.084638 logp=-2.469367\n",
            "07 token=' movie'     p=0.036412 logp=-3.312869\n",
            "08 token=' was'       p=0.296338 logp=-1.216254\n",
            "09 token=' a'         p=0.067677 logp=-2.693010\n",
            "10 token=' joke'      p=0.173508 logp=-1.751529\n",
            "11 token='.'          p=0.280387 logp=-1.271584\n",
            "12 token=' It'        p=0.123003 logp=-2.095544\n",
            "13 token=' was'       p=0.519723 logp=-0.654460\n",
            "14 token=' a'         p=0.149313 logp=-1.901710\n",
            "15 token=' joke'      p=0.268988 logp=-1.313089\n",
            "16 token=' about'     p=0.424149 logp=-0.857670\n",
            "17 token=' how'       p=0.174166 logp=-1.747744\n",
            "18 token=' the'       p=0.123646 logp=-2.090337\n",
            "19 token=' movie'     p=0.616074 logp=-0.484388\n",
            "20 token=' was'       p=0.634955 logp=-0.454201\n",
            "21 token=' a'         p=0.409556 logp=-0.892682\n",
            "22 token=' joke'      p=0.737557 logp=-0.304413\n",
            "23 token='.'          p=0.459488 logp=-0.777644\n",
            "24 token=' It'        p=0.238917 logp=-1.431638\n",
            "25 token=' was'       p=0.648174 logp=-0.433596\n",
            "26 token=' a'         p=0.487548 logp=-0.718367\n",
            "27 token=' joke'      p=0.752988 logp=-0.283706\n",
            "28 token=' about'     p=0.902346 logp=-0.102757\n",
            "29 token=' how'       p=0.722222 logp=-0.325422\n",
            "30 token=' the'       p=0.530318 logp=-0.634278\n",
            "31 token=' movie'     p=0.862762 logp=-0.147617\n",
            "32 token=' was'       p=0.861974 logp=-0.148530\n",
            "33 token=' a'         p=0.801219 logp=-0.221622\n",
            "34 token=' joke'      p=0.902633 logp=-0.102439\n",
            "35 token='.'          p=0.853989 logp=-0.157838\n",
            "36 token=' It'        p=0.455482 logp=-0.786399\n",
            "37 token=' was'       p=0.828501 logp=-0.188138\n",
            "38 token=' a'         p=0.883896 logp=-0.123416\n",
            "39 token=' joke'      p=0.912144 logp=-0.091957\n",
            "40 token=' about'     p=0.930617 logp=-0.071908\n",
            "41 token=' how'       p=0.929194 logp=-0.073438\n",
            "42 token=' the'       p=0.892337 logp=-0.113912\n",
            "43 token=' movie'     p=0.945375 logp=-0.056174\n",
            "44 token=' was'       p=0.955032 logp=-0.046010\n",
            "45 token=' a'         p=0.967734 logp=-0.032798\n",
            "46 token=' joke'      p=0.986991 logp=-0.013094\n",
            "47 token='.'          p=0.953991 logp=-0.047101\n",
            "48 token=' It'        p=0.658382 logp=-0.417969\n",
            "49 token=' was'       p=0.936970 logp=-0.065104\n",
            "50 token=' a'         p=0.962934 logp=-0.037770\n"
          ]
        }
      ]
    }
  ]
}