{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOx5wnUX7BX/3j3/Eyz+Hcf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haru1489248/nlp-100-nock/blob/main/ch10/section_98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 98. ファインチューニング\n",
        "問題96のプロンプトに対して、正解の感情ラベルをテキストの応答として返すように事前学習済みモデルをファインチューニングせよ。\n",
        "chat template を使用したinputは長すぎるので、今回は使用しなかった（スペックが足りない）"
      ],
      "metadata": {
        "id": "F_o7LD8vw1wZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuGVWUQkw063",
        "outputId": "1587c5de-eb55-4e70-f1ec-843b7ee855ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.2)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading transformers-5.1.0-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers, evaluate\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 5.0.0\n",
            "    Uninstalling transformers-5.0.0:\n",
            "      Successfully uninstalled transformers-5.0.0\n",
            "Successfully installed evaluate-0.4.6 transformers-5.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from typing import Any, Tuple, Union, Optional\n",
        "from datasets import Dataset\n",
        "# parameter efficient fine-tuning module import\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2dM4U6txHM6",
        "outputId": "61898689-126e-4570-d429-2b6f30e271e0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalLMDataCollator:\n",
        "  def __init__(self, tokenizer: Any, label_pad_token_id: int):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.label_pad_token_id = label_pad_token_id\n",
        "\n",
        "  def __call__(self, features: list[dict[str, Union[str, int]]]) -> dict[str, torch.Tensor]:\n",
        "    labels = [f[\"labels\"] for f in features]\n",
        "    for f in features:\n",
        "      f.pop(\"labels\")\n",
        "\n",
        "    batch = self.tokenizer.pad(\n",
        "        features,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    max_len = batch[\"input_ids\"].shape[1]\n",
        "    padded_labels = []\n",
        "    for l in labels:\n",
        "      l = l[:max_len]\n",
        "      padded = l + [self.label_pad_token_id] * (max_len - len(l))\n",
        "      padded_labels.append(padded)\n",
        "\n",
        "    batch[\"labels\"] = torch.tensor(padded_labels, dtype=torch.long)\n",
        "    return batch"
      ],
      "metadata": {
        "id": "Ze_ZhiKfL0PK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"llm-jp/llm-jp-3-150m-instruct3\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dev_src = \"/content/drive/MyDrive/SST-2/dev.tsv\"\n",
        "train_src = \"/content/drive/MyDrive/SST-2/train.tsv\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.padding_side = \"left\"\n",
        "if tokenizer.pad_token_id is None:\n",
        "  tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "5TbS2qY7x0AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "POS_ID = tokenizer(\"positive\", add_special_tokens=False)[\"input_ids\"]\n",
        "NEG_ID = tokenizer(\"negative\", add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "# assert 条件, エラー出力データ\n",
        "# assert は条件がfalseの場合はエラーを出力する\n",
        "assert len(POS_ID) == 1 and len(NEG_ID) == 1, (POS_ID, NEG_ID)\n",
        "\n",
        "POS_ID = POS_ID[0]\n",
        "NEG_ID = NEG_ID[0]"
      ],
      "metadata": {
        "id": "SSR5-L03yW2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_logits_for_metrics(logits: Union[torch.Tensor, tuple], labels: Optional[torch.Tensor]) -> torch.Tensor:\n",
        "  if isinstance(logits, tuple):\n",
        "    logits = logits[0]\n",
        "  pos = logits[:, :, POS_ID] # shape = (batch_size, seq_len)\n",
        "  neg = logits[:, :, NEG_ID]\n",
        "  return torch.stack([pos, neg], dim=-1) # 末尾に新しくvocab_sizeの代わりに入れる"
      ],
      "metadata": {
        "id": "4NphDoZskhQ2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(eval_pred: Tuple[np.ndarray, np.ndarray]) -> dict[str, float]:\n",
        "  logits, labels = eval_pred\n",
        "  preds, refs = [], []\n",
        "\n",
        "  for i in range(labels.shape[0]):\n",
        "    idxs = np.where(labels[i] != -100)[0]\n",
        "    if len(idxs) == 0:\n",
        "      continue\n",
        "    t = idxs[0]\n",
        "\n",
        "    pos_score = logits[i, t, 0] # vocab_sizeは2で0番目はpositive\n",
        "    neg_score = logits[i, t, 1]\n",
        "    pred_label = 1 if pos_score > neg_score else 0\n",
        "\n",
        "    gold_token = labels[i, t]\n",
        "    gold_label = 1 if gold_token == POS_ID else 0\n",
        "\n",
        "    preds.append(pred_label)\n",
        "    refs.append(gold_label)\n",
        "\n",
        "  return metric.compute(predictions=preds, references=refs)"
      ],
      "metadata": {
        "id": "sYRY6E_-nPCJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main() -> None:\n",
        "  train_dataset = Dataset.from_csv(train_src, sep=\"\\t\")\n",
        "  dev_dataset = Dataset.from_csv(dev_src, sep=\"\\t\")\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_id,\n",
        "      device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "  )\n",
        "\n",
        "  peft_config = LoraConfig(\n",
        "      task_type=TaskType.CAUSAL_LM,\n",
        "      inference_mode=False, # 配布されているものを使用するときはTrueらしい\n",
        "      r=8,\n",
        "      lora_alpha=16,\n",
        "      lora_dropout=0.1, # LoRAの部分だけ1割の確率でドロップアウトさせる\n",
        "      target_modules=[\n",
        "          \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", # query key value ouput\n",
        "          \"gate_proj\", \"up_proj\", \"down_proj\" # 層の名前だけどわからなかった\n",
        "      ],\n",
        "  )\n",
        "\n",
        "  model = get_peft_model(model, peft_config=peft_config)\n",
        "  model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "  def tokenize_function(examples):\n",
        "    prompts = []\n",
        "    answers = []\n",
        "    for sentence, label in zip(examples[\"sentence\"], examples[\"label\"]):\n",
        "         prompt = f\"次の文をpositiveかnegativeで答えて。\\n文: {sentence}\\nラベル:\"\n",
        "\n",
        "         # 終了がわかりやすい方が精度上がりやすいらしい？\n",
        "         ans = (\"positive\" if int(label) == 1 else \"negative\") + tokenizer.eos_token\n",
        "\n",
        "         prompts.append(prompt)\n",
        "         answers.append(ans)\n",
        "\n",
        "    prompt_token = tokenizer(\n",
        "        prompts,\n",
        "        padding=False, # 後でdatacollatorでバッチごとにpaddingしたいのでFalse\n",
        "        max_length=64,\n",
        "        truncation=True,\n",
        "        add_special_tokens=False\n",
        "    )\n",
        "\n",
        "    answer_token = tokenizer(\n",
        "        answers,\n",
        "        add_special_tokens=False,\n",
        "        padding=False,\n",
        "    )\n",
        "\n",
        "    input_ids, attention_mask, labels = [], [], []\n",
        "\n",
        "    for prompt_ids, answer_ids in zip(prompt_token[\"input_ids\"], answer_token[\"input_ids\"]):\n",
        "      ids = prompt_ids + answer_ids\n",
        "      input_ids.append(ids)\n",
        "      attention_mask.append([1] * len(ids))\n",
        "\n",
        "      # ignore loss on prompt tokens\n",
        "      labels.append([-100] * len(prompt_ids) + answer_ids)\n",
        "\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "  # batched=Trueでバッチごとにfunctionにデータを渡す\n",
        "  train_data = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names) # 必要のな区なったカラムを消す\n",
        "  dev_data = dev_dataset.map(tokenize_function, batched=True, remove_columns=dev_dataset.column_names)\n",
        "\n",
        "  datacollator = CausalLMDataCollator(\n",
        "      tokenizer=tokenizer,\n",
        "      label_pad_token_id=-100,\n",
        "      )\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "        output_dir=\"./results_98ioynb\",\n",
        "        num_train_epochs=1, # データを何周するか\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=2,\n",
        "        learning_rate=2e-4, # 2 * 10^{-4}: 0.0002\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        warmup_ratio=0.1,\n",
        "        eval_strategy=\"epoch\", # 評価をいつ実行するか決める\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        save_only_model=True,\n",
        "        eval_accumulation_steps=2,  # 10ステップごとに結果をCPUへ移動させる\n",
        "  )\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train_data, # データが多過ぎて学習に時間がかかるので今回は少なくした\n",
        "      eval_dataset=dev_data,\n",
        "      data_collator=datacollator,\n",
        "      compute_metrics=compute_accuracy,\n",
        "      preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "\n",
        "  eval_results = trainer.evaluate()\n",
        "  print(f\"Accuracy (dev dataset): {eval_results}\")"
      ],
      "metadata": {
        "id": "5hm_4yyUzCWd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "XP4PpHxKzFjc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}