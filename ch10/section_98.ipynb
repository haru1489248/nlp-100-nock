{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9epU5iANP7OIv+08MfzJO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haru1489248/nlp-100-nock/blob/main/ch10/section_98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 98. ファインチューニング\n",
        "問題96のプロンプトに対して、正解の感情ラベルをテキストの応答として返すように事前学習済みモデルをファインチューニングせよ。"
      ],
      "metadata": {
        "id": "F_o7LD8vw1wZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuGVWUQkw063",
        "outputId": "9a996803-5fe2-42eb-a724-7529a5365c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.2)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading transformers-5.1.0-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers, evaluate\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 5.0.0\n",
            "    Uninstalling transformers-5.0.0:\n",
            "      Successfully uninstalled transformers-5.0.0\n",
            "Successfully installed evaluate-0.4.6 transformers-5.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from typing import Any, Tuple, Union\n",
        "from datasets import Dataset\n",
        "# parameter efficient fine-tuning module import\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    GenerationConfig\n",
        ")\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2dM4U6txHM6",
        "outputId": "90028247-a7aa-4fd1-b6ce-78c5c9594157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TENSORBOARD_LOGGING_DIR\"] = \"./logs\"\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dev_src = \"/content/drive/MyDrive/SST-2/dev.tsv\"\n",
        "train_src = \"/content/drive/MyDrive/SST-2/train.tsv\""
      ],
      "metadata": {
        "id": "5TbS2qY7x0AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(eval_pred: Tuple[np.array, np.array]) -> dict[str, float]:\n",
        "  metric = evaluate.load(\"accuracy\")\n",
        "  pred, labels = eval_pred\n",
        "  preds = pred.argmax(axis=1)\n",
        "  return metric.compute(predictions=pred, references=labels)"
      ],
      "metadata": {
        "id": "SSR5-L03yW2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main() -> None:\n",
        "  train_dataset = Dataset.from_csv(train_src, sep=\"\\t\")\n",
        "  dev_dataset = Dataset.from_csv(dev_src, sep=\"\\t\")\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
        "  if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "  generation_config = GenerationConfig(\n",
        "      max_new_tokens=10, # positiveかnegativeだけでいいので少なめに設定\n",
        "      pad_token_id=tokenizer.pad_token_id,\n",
        "      eos_token_id=tokenizer.eos_token_id,\n",
        "      do_sample=False # gready\n",
        "  )\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_id,\n",
        "      device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "  )\n",
        "\n",
        "  peft_config = LoraConfig(\n",
        "      task_type=TaskType.CAUSAL_LM,\n",
        "      inference_mode=False, # 配布されているものを使用するときはTrueらしい\n",
        "      r=8,\n",
        "      lora_alpha=16,\n",
        "      lora_dropout=0.1 # LoRAの部分だけ1割の確率でドロップアウトさせる\n",
        "  )\n",
        "\n",
        "  model = get_peft_model(model, peft_config=peft_config)\n",
        "\n",
        "  def tokenize_function(examples):\n",
        "    prompts = []\n",
        "    answers = []\n",
        "    for sentence, label in zip(examples[\"sentence\"], examples[\"labels\"]):\n",
        "         messages = [\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": \"\"\"\n",
        "              You are a classification model for the sentiment analyzer.\n",
        "              Answer with exactly one word: positive or negative.\n",
        "              Do not output anything else.\n",
        "              For example, the positive sentence 'The movie was full of fan.' is inputted, you should return positive.\n",
        "              \"\"\"\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": sentence\n",
        "          }\n",
        "         ]\n",
        "\n",
        "         prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "         ans = \"positive\" if int(label) == 1 else \"negative\"\n",
        "\n",
        "         prompts.append(prompt)\n",
        "         answers.append(ans)\n",
        "\n",
        "    prompt_token = tokenizer(\n",
        "        prompts,\n",
        "        padding=False, # 後でdatacollatorでバッチごとにpaddingしたいのでFalse\n",
        "        padding_side=\"left\",\n",
        "        return_tensors=\"pt\",\n",
        "        add_special_tokens=False\n",
        "    )\n",
        "\n",
        "    answer_token = tokenizer(\n",
        "        answers,\n",
        "        add_special_tokens=False,\n",
        "        padding=False,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    input_ids, attention_mask, labels = [], [], []\n",
        "\n",
        "    for prompt_ids, answer_ids in zip(prompt_token[\"input_ids\"], answer_token[\"input_ids\"]):\n",
        "      ids = prompt_ids + answer_ids\n",
        "      input_ids.append(ids)\n",
        "      attention_mask.append([1] * len(ids))\n",
        "\n",
        "      # ignore loss on prompt tokens\n",
        "      labels.append([-100] * len(p_ids) + a_ids)\n",
        "\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "# batched=Trueでバッチごとにfunctionにデータを渡す\n",
        "train_data = train_dataset.map(tokenize_function, batched=True)\n",
        "dev_data = dev_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "datacollator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "      output_dir=\"./results_97ioynb\",\n",
        "      num_train_epochs=2, # データを何周するか\n",
        "      per_device_eval_batch_size=32,\n",
        "      per_device_train_batch_size=32,\n",
        "      learning_rate=2e-4, # 2 * 10^{-4}: 0.0002\n",
        "      lr_scheduler_type=\"linear\",\n",
        "      warmup_ratio=0.1,\n",
        "      eval_strategy=\"epoch\", # 評価をいつ実行するか決める\n",
        "      save_strategy=\"epoch\",\n",
        "      load_best_model_at_end=True,\n",
        "      metric_for_best_model=\"accuracy\",\n",
        "      fp16=True,\n",
        "      save_only_model=True,\n",
        "      report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=dev_data,\n",
        "    data_collator=datacollator,\n",
        "    compute_metrics=compute_accuracy\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Accuracy (dev dataset): {eval_results}\")"
      ],
      "metadata": {
        "id": "5hm_4yyUzCWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "XP4PpHxKzFjc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}