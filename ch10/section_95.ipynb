{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPY9lE1VOchgWxi8P5MIWjJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haru1489248/nlp-100-nock/blob/main/ch10/section_95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 95. マルチターンのチャット\n",
        "問題94で生成された応答に対して、追加で”Please give me the plural form of the word with its spelling in reverse order.”と問いかけたときの応答を生成・表示せよ。また、その時に言語モデルに与えるプロンプトを確認せよ。\n",
        "\n",
        "### チャットテンプレートとは？\n",
        "会話形式で書いたやり取りを、モデルが実際に読むプロンプトに変換するための定型フォーマットのこと\n",
        "\n",
        "### modelのrole\n",
        "- system: ルール\n",
        "- user: 質問\n",
        "- assistant: 生成されるもの"
      ],
      "metadata": {
        "id": "VGaH_lI3cmbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "MDF8v7Cs5vty"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkOJhT_FchKD",
        "outputId": "ea376b54-c587-4aa5-a1ff-9777adee018d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
      ],
      "metadata": {
        "id": "ZIZry8jhcxmO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "chat = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\"},\n",
        "    {\"role\": \"user\", \"content\": \"What do you call a sweet eaten after dinner?\"},\n",
        "]\n",
        "max_new_tokens = 50"
      ],
      "metadata": {
        "id": "17RcXqkGc8oq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "if tokenizer.pad_token_id is None:\n",
        "  tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "ws4yqmRRdI_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "_oQPPSNgdQrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tokenizer.apply_chat_templateとは？\n",
        "chat形式（role付きの辞書）をモデルが読める形式に変換する関数\n",
        "- tokenize=False(デフォルト): これは文字列（プロンプトを返す）\n",
        "```\n",
        "<|system|> You are a pirate\n",
        "<|user|> What do you call...\n",
        "<|assistant|>\n",
        "```\n",
        "  -  Trueの場合はトークン化して返す\n",
        "### add_generation_prompt=Trueとは？\n",
        "assistantが追加されてモデルが生成したものを出力できる様になる"
      ],
      "metadata": {
        "id": "qFif0vHjXZEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_chat = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
        "prompt_len = tokenize_chat[\"input_ids\"].shape[-1]"
      ],
      "metadata": {
        "id": "iBYG03KMdTEk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")"
      ],
      "metadata": {
        "id": "2h1gZR5G4LcD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(**tokenize_chat, generation_config=generation_config)"
      ],
      "metadata": {
        "id": "dasNE2F05bu4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "入力も返されるので、プロンプトの長さでマスクする"
      ],
      "metadata": {
        "id": "59UIG50QY_UF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shifted_sentence = output[0][prompt_len:]\n",
        "print(shifted_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fy82NshNW7Zn",
        "outputId": "67eef03e-b9ec-4b02-b271-81d3e51c6c64"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([   56,   261,  1427,   258,     6, 18728,   264, 32726,    11, 36346,\n",
            "           30, 98693,  1243,    11, 30276,    88,     0,   358,   387,  1440,\n",
            "          258,     6,   279,  4320,   311, 55295,  3488,    13, 32269,   387,\n",
            "         3137,   258,     6,   297,     6,   264,  1131,   320,    67,  2453,\n",
            "          780, 18579,     8,  2564,  5701,   634,   258,     6,     0,   362])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_1 = tokenizer.decode(shifted_sentence, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "55YASS9Q5n1H"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB2JX29s5rRZ",
        "outputId": "5b3fbde9-9466-425d-e4fe-2703f318dfcd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yer lookin' fer a treasure, eh? Alright then, matey! I be knowin' the answer to yer question. Ye be talkin' o' a... (dramatic pause)...puddin'! A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.append({ \"role\": \"assistant\", \"content\": answer_1 })\n",
        "chat.append({ \"role\": \"user\", \"content\": \"Please give me the plural form of the word with its spelling in reverse order.\" })"
      ],
      "metadata": {
        "id": "YWQivIC9alp5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(chat)):\n",
        "  print(chat[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P17ArHS0dtyD",
        "outputId": "a9f5e9cc-f2ab-4fac-87a3-f658dfbb07ae"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'role': 'system', 'content': 'You are a friendly chatbot who always responds in the style of a pirate'}\n",
            "{'role': 'user', 'content': 'What do you call a sweet eaten after dinner?'}\n",
            "{'role': 'assistant', 'content': \"Yer lookin' fer a treasure, eh? Alright then, matey! I be knowin' the answer to yer question. Ye be talkin' o' a... (dramatic pause)...puddin'! A\"}\n",
            "{'role': 'user', 'content': 'Please give me the plural form of the word with its spelling in reverse order.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
        "prompt_len = prompt_text[\"input_ids\"].shape[-1]"
      ],
      "metadata": {
        "id": "pQT8HIEvc9dM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt_text)\n",
        "print(prompt_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkzMWmAzdPVV",
        "outputId": "33f61e6c-0114-4089-b429-0be508cce71c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
            "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
            "            220,   2705,  13806,    220,   2366,     21,    271,   2675,    527,\n",
            "            264,  11919,   6369,   6465,    889,   2744,  31680,    304,    279,\n",
            "           1742,    315,    264,  55066, 128009, 128006,    882, 128007,    271,\n",
            "           3923,    656,    499,   1650,    264,  10437,  35661,   1306,  14177,\n",
            "             30, 128009, 128006,  78191, 128007,    271,     56,    261,   1427,\n",
            "            258,      6,  18728,    264,  32726,     11,  36346,     30,  98693,\n",
            "           1243,     11,  30276,     88,      0,    358,    387,   1440,    258,\n",
            "              6,    279,   4320,    311,  55295,   3488,     13,  32269,    387,\n",
            "           3137,    258,      6,    297,      6,    264,   1131,    320,     67,\n",
            "           2453,    780,  18579,  40125,   5701,    634,    258,      6,      0,\n",
            "            362, 128009, 128006,    882, 128007,    271,   5618,   3041,    757,\n",
            "            279,  39598,   1376,    315,    279,   3492,    449,   1202,  43529,\n",
            "            304,  10134,   2015,     13, 128009, 128006,  78191, 128007,    271]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**prompt_text, generation_config=generation_config)"
      ],
      "metadata": {
        "id": "U5WfjQ33dVZg"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N3Nj-FIedlZ",
        "outputId": "2da03c83-050b-438e-e488-83e3c0ac36a3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
            "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
            "            220,   2705,  13806,    220,   2366,     21,    271,   2675,    527,\n",
            "            264,  11919,   6369,   6465,    889,   2744,  31680,    304,    279,\n",
            "           1742,    315,    264,  55066, 128009, 128006,    882, 128007,    271,\n",
            "           3923,    656,    499,   1650,    264,  10437,  35661,   1306,  14177,\n",
            "             30, 128009, 128006,  78191, 128007,    271,     56,    261,   1427,\n",
            "            258,      6,  18728,    264,  32726,     11,  36346,     30,  98693,\n",
            "           1243,     11,  30276,     88,      0,    358,    387,   1440,    258,\n",
            "              6,    279,   4320,    311,  55295,   3488,     13,  32269,    387,\n",
            "           3137,    258,      6,    297,      6,    264,   1131,    320,     67,\n",
            "           2453,    780,  18579,  40125,   5701,    634,    258,      6,      0,\n",
            "            362, 128009, 128006,    882, 128007,    271,   5618,   3041,    757,\n",
            "            279,  39598,   1376,    315,    279,   3492,    449,   1202,  43529,\n",
            "            304,  10134,   2015,     13, 128009, 128006,  78191, 128007,    271,\n",
            "          87575,    387,   1390,    258,      6,    279,  39598,    297,      6,\n",
            "            330,   5701,    634,    258,  15260,    449,   1202,  43529,    304,\n",
            "          10134,     11,  36346,     30,  98693,   1243,     11,  30276,     88,\n",
            "              0,    578,   4320,    387,   1131,    330,   5701,    634,   1354,\n",
            "              1,      0, 128009]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shifted_sentence = outputs[0][prompt_len:]\n",
        "print(shifted_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6JD7Q_Uee2y",
        "outputId": "43615d03-c196-473d-ad26-9cd18e3aec18"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 87575,    387,   1390,    258,      6,    279,  39598,    297,      6,\n",
            "           330,   5701,    634,    258,  15260,    449,   1202,  43529,    304,\n",
            "         10134,     11,  36346,     30,  98693,   1243,     11,  30276,     88,\n",
            "             0,    578,   4320,    387,   1131,    330,   5701,    634,   1354,\n",
            "             1,      0, 128009])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_2 = tokenizer.decode(shifted_sentence, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "-t74Fd42eqJv"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtRUTFbce2Yo",
        "outputId": "b6e472eb-c20d-4c92-aa24-a2c9e99e1eb9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ye be wantin' the plural o' \"puddin'\" with its spelling in reverse, eh? Alright then, matey! The answer be... \"puddins\"!\n"
          ]
        }
      ]
    }
  ]
}